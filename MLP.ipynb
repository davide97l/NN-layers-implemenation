{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "homework2-mlp.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wgtcbnv6rdkO",
        "colab_type": "text"
      },
      "source": [
        "# Homework-2: MLP for MNIST Classification\n",
        "\n",
        "### In this homework, you need to\n",
        "- #### implement SGD optimizer (`./optimizer.py`)\n",
        "- #### implement forward and backward for FCLayer (`layers/fc_layer.py`)\n",
        "- #### implement forward and backward for SigmoidLayer (`layers/sigmoid_layer.py`)\n",
        "- #### implement forward and backward for ReLULayer (`layers/relu_layer.py`)\n",
        "- #### implement EuclideanLossLayer (`criterion/euclidean_loss.py`)\n",
        "- #### implement SoftmaxCrossEntropyLossLayer (`criterion/softmax_cross_entropy.py`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHsV9MrHriVy",
        "colab_type": "code",
        "outputId": "94862a97-17a2-48cb-9dcf-e1b2c0e9d2be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq7pntYDrqgR",
        "colab_type": "code",
        "outputId": "58a430b1-56f9-416b-9561-c0f742421c7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/homework2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/homework2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukVgbPgLrdkR",
        "colab_type": "code",
        "outputId": "f2de0468-40e9-4253-d5c2-e01edab141a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "\n",
        "from network import Network\n",
        "from solver import train, test\n",
        "from plot import plot_loss_and_acc"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT405gYxrdkW",
        "colab_type": "text"
      },
      "source": [
        "## Load MNIST Dataset\n",
        "We use tensorflow tools to load dataset for convenience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoM1Lso-rdkX",
        "colab_type": "code",
        "outputId": "d19d529d-668f-4667-d816-7d90681a2775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGjSK42HrdkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_image(image):\n",
        "    # Normalize from [0, 255.] to [0., 1.0], and then subtract by the mean value\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = tf.reshape(image, [784])\n",
        "    image = image / 255.0\n",
        "    image = image - tf.reduce_mean(image)\n",
        "    return image\n",
        "\n",
        "def decode_label(label):\n",
        "    # Encode label with one-hot encoding\n",
        "    return tf.one_hot(label, depth=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZGrBjY2rdkc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Preprocessing\n",
        "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
        "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
        "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
        "\n",
        "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
        "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
        "data_test = tf.data.Dataset.zip((x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4fSS8iZrdkf",
        "colab_type": "text"
      },
      "source": [
        "## Set Hyerparameters\n",
        "You can modify hyerparameters by yourself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Yly-bEhrdkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 100\n",
        "max_epoch = 20\n",
        "init_std = 0.01\n",
        "\n",
        "learning_rate_SGD = 0.001\n",
        "weight_decay = 0.1\n",
        "\n",
        "disp_freq = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zieJO3S6rdkl",
        "colab_type": "text"
      },
      "source": [
        "## 1. MLP with Euclidean Loss\n",
        "In part-1, you need to train a MLP with **Euclidean Loss**.  \n",
        "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively.\n",
        "### TODO\n",
        "Before executing the following code, you should complete **./optimizer.py** and **criterion/euclidean_loss.py**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojM3ExETrdkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from criterion import EuclideanLossLayer\n",
        "from optimizer import SGD\n",
        "\n",
        "criterion = EuclideanLossLayer()\n",
        "\n",
        "sgd = SGD(learning_rate_SGD, weight_decay)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGfXP-7vrdkp",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 MLP with Euclidean Loss and Sigmoid Activation Function\n",
        "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Euclidean loss function.\n",
        "\n",
        "### TODO\n",
        "Before executing the following code, you should complete **layers/fc_layer.py** and **layers/sigmoid_layer.py**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqmpjZyirdkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from layers import FCLayer, SigmoidLayer\n",
        "\n",
        "sigmoidMLP = Network()\n",
        "# Build MLP with FCLayer and SigmoidLayer\n",
        "# 128 is the number of hidden units, you can change by your own\n",
        "sigmoidMLP.add(FCLayer(784, 128))\n",
        "sigmoidMLP.add(SigmoidLayer())\n",
        "sigmoidMLP.add(FCLayer(128, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "EheWyO43rdkt",
        "colab_type": "code",
        "outputId": "a2505d55-3c84-42f1-c542-105316253180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/drive/My Drive/homework2/solver.py:14: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/homework2/solver.py:15: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch [0][20]\t Batch [0][550]\t Training Loss 478.0000\t Accuracy 0.0600\n",
            "Epoch [0][20]\t Batch [50][550]\t Training Loss 545.5686\t Accuracy 0.1725\n",
            "Epoch [0][20]\t Batch [100][550]\t Training Loss 485.3020\t Accuracy 0.2593\n",
            "Epoch [0][20]\t Batch [150][550]\t Training Loss 449.7185\t Accuracy 0.3412\n",
            "Epoch [0][20]\t Batch [200][550]\t Training Loss 421.9328\t Accuracy 0.4095\n",
            "Epoch [0][20]\t Batch [250][550]\t Training Loss 400.9323\t Accuracy 0.4602\n",
            "Epoch [0][20]\t Batch [300][550]\t Training Loss 379.0548\t Accuracy 0.4992\n",
            "Epoch [0][20]\t Batch [350][550]\t Training Loss 361.1282\t Accuracy 0.5309\n",
            "Epoch [0][20]\t Batch [400][550]\t Training Loss 342.7506\t Accuracy 0.5585\n",
            "Epoch [0][20]\t Batch [450][550]\t Training Loss 327.9058\t Accuracy 0.5812\n",
            "Epoch [0][20]\t Batch [500][550]\t Training Loss 314.9112\t Accuracy 0.6013\n",
            "\n",
            "Epoch [0]\t Average training loss 301.9464\t Average training accuracy 0.6196\n",
            "Epoch [0]\t Average validation loss 148.8000\t Average validation accuracy 0.8376\n",
            "\n",
            "Epoch [1][20]\t Batch [0][550]\t Training Loss 149.5000\t Accuracy 0.8200\n",
            "Epoch [1][20]\t Batch [50][550]\t Training Loss 166.0490\t Accuracy 0.8106\n",
            "Epoch [1][20]\t Batch [100][550]\t Training Loss 158.5891\t Accuracy 0.8193\n",
            "Epoch [1][20]\t Batch [150][550]\t Training Loss 162.7318\t Accuracy 0.8162\n",
            "Epoch [1][20]\t Batch [200][550]\t Training Loss 160.5124\t Accuracy 0.8185\n",
            "Epoch [1][20]\t Batch [250][550]\t Training Loss 158.4522\t Accuracy 0.8222\n",
            "Epoch [1][20]\t Batch [300][550]\t Training Loss 154.1877\t Accuracy 0.8264\n",
            "Epoch [1][20]\t Batch [350][550]\t Training Loss 151.2151\t Accuracy 0.8290\n",
            "Epoch [1][20]\t Batch [400][550]\t Training Loss 147.5686\t Accuracy 0.8325\n",
            "Epoch [1][20]\t Batch [450][550]\t Training Loss 145.7871\t Accuracy 0.8347\n",
            "Epoch [1][20]\t Batch [500][550]\t Training Loss 144.5549\t Accuracy 0.8365\n",
            "\n",
            "Epoch [1]\t Average training loss 142.8718\t Average training accuracy 0.8385\n",
            "Epoch [1]\t Average validation loss 97.7300\t Average validation accuracy 0.8988\n",
            "\n",
            "Epoch [2][20]\t Batch [0][550]\t Training Loss 82.5000\t Accuracy 0.9200\n",
            "Epoch [2][20]\t Batch [50][550]\t Training Loss 112.9510\t Accuracy 0.8688\n",
            "Epoch [2][20]\t Batch [100][550]\t Training Loss 115.4307\t Accuracy 0.8685\n",
            "Epoch [2][20]\t Batch [150][550]\t Training Loss 121.5430\t Accuracy 0.8637\n",
            "Epoch [2][20]\t Batch [200][550]\t Training Loss 121.5299\t Accuracy 0.8652\n",
            "Epoch [2][20]\t Batch [250][550]\t Training Loss 121.1155\t Accuracy 0.8667\n",
            "Epoch [2][20]\t Batch [300][550]\t Training Loss 118.9186\t Accuracy 0.8679\n",
            "Epoch [2][20]\t Batch [350][550]\t Training Loss 117.7821\t Accuracy 0.8681\n",
            "Epoch [2][20]\t Batch [400][550]\t Training Loss 115.7980\t Accuracy 0.8696\n",
            "Epoch [2][20]\t Batch [450][550]\t Training Loss 115.2816\t Accuracy 0.8704\n",
            "Epoch [2][20]\t Batch [500][550]\t Training Loss 115.4830\t Accuracy 0.8706\n",
            "\n",
            "Epoch [2]\t Average training loss 115.0245\t Average training accuracy 0.8712\n",
            "Epoch [2]\t Average validation loss 89.3900\t Average validation accuracy 0.9126\n",
            "\n",
            "Epoch [3][20]\t Batch [0][550]\t Training Loss 78.0000\t Accuracy 0.9300\n",
            "Epoch [3][20]\t Batch [50][550]\t Training Loss 95.2353\t Accuracy 0.8902\n",
            "Epoch [3][20]\t Batch [100][550]\t Training Loss 100.6287\t Accuracy 0.8873\n",
            "Epoch [3][20]\t Batch [150][550]\t Training Loss 106.5695\t Accuracy 0.8823\n",
            "Epoch [3][20]\t Batch [200][550]\t Training Loss 105.8806\t Accuracy 0.8831\n",
            "Epoch [3][20]\t Batch [250][550]\t Training Loss 105.5100\t Accuracy 0.8843\n",
            "Epoch [3][20]\t Batch [300][550]\t Training Loss 103.7176\t Accuracy 0.8850\n",
            "Epoch [3][20]\t Batch [350][550]\t Training Loss 103.7436\t Accuracy 0.8842\n",
            "Epoch [3][20]\t Batch [400][550]\t Training Loss 102.6434\t Accuracy 0.8847\n",
            "Epoch [3][20]\t Batch [450][550]\t Training Loss 102.6563\t Accuracy 0.8852\n",
            "Epoch [3][20]\t Batch [500][550]\t Training Loss 102.9092\t Accuracy 0.8849\n",
            "\n",
            "Epoch [3]\t Average training loss 102.8364\t Average training accuracy 0.8852\n",
            "Epoch [3]\t Average validation loss 79.0700\t Average validation accuracy 0.9180\n",
            "\n",
            "Epoch [4][20]\t Batch [0][550]\t Training Loss 85.5000\t Accuracy 0.9300\n",
            "Epoch [4][20]\t Batch [50][550]\t Training Loss 88.4216\t Accuracy 0.9010\n",
            "Epoch [4][20]\t Batch [100][550]\t Training Loss 94.2426\t Accuracy 0.8965\n",
            "Epoch [4][20]\t Batch [150][550]\t Training Loss 98.6987\t Accuracy 0.8921\n",
            "Epoch [4][20]\t Batch [200][550]\t Training Loss 98.2338\t Accuracy 0.8930\n",
            "Epoch [4][20]\t Batch [250][550]\t Training Loss 97.7012\t Accuracy 0.8937\n",
            "Epoch [4][20]\t Batch [300][550]\t Training Loss 96.3256\t Accuracy 0.8942\n",
            "Epoch [4][20]\t Batch [350][550]\t Training Loss 96.8063\t Accuracy 0.8932\n",
            "Epoch [4][20]\t Batch [400][550]\t Training Loss 96.0711\t Accuracy 0.8934\n",
            "Epoch [4][20]\t Batch [450][550]\t Training Loss 95.4435\t Accuracy 0.8940\n",
            "Epoch [4][20]\t Batch [500][550]\t Training Loss 95.7794\t Accuracy 0.8936\n",
            "\n",
            "Epoch [4]\t Average training loss 95.9909\t Average training accuracy 0.8936\n",
            "Epoch [4]\t Average validation loss 76.2000\t Average validation accuracy 0.9232\n",
            "\n",
            "Epoch [5][20]\t Batch [0][550]\t Training Loss 71.0000\t Accuracy 0.9500\n",
            "Epoch [5][20]\t Batch [50][550]\t Training Loss 83.4804\t Accuracy 0.9078\n",
            "Epoch [5][20]\t Batch [100][550]\t Training Loss 88.4604\t Accuracy 0.9028\n",
            "Epoch [5][20]\t Batch [150][550]\t Training Loss 92.9205\t Accuracy 0.8992\n",
            "Epoch [5][20]\t Batch [200][550]\t Training Loss 92.3806\t Accuracy 0.9001\n",
            "Epoch [5][20]\t Batch [250][550]\t Training Loss 91.4562\t Accuracy 0.9008\n",
            "Epoch [5][20]\t Batch [300][550]\t Training Loss 90.7824\t Accuracy 0.9009\n",
            "Epoch [5][20]\t Batch [350][550]\t Training Loss 91.0370\t Accuracy 0.9000\n",
            "Epoch [5][20]\t Batch [400][550]\t Training Loss 90.7057\t Accuracy 0.8999\n",
            "Epoch [5][20]\t Batch [450][550]\t Training Loss 90.2838\t Accuracy 0.9001\n",
            "Epoch [5][20]\t Batch [500][550]\t Training Loss 90.8713\t Accuracy 0.8995\n",
            "\n",
            "Epoch [5]\t Average training loss 91.1609\t Average training accuracy 0.8995\n",
            "Epoch [5]\t Average validation loss 73.2600\t Average validation accuracy 0.9278\n",
            "\n",
            "Epoch [6][20]\t Batch [0][550]\t Training Loss 71.0000\t Accuracy 0.9500\n",
            "Epoch [6][20]\t Batch [50][550]\t Training Loss 75.9510\t Accuracy 0.9141\n",
            "Epoch [6][20]\t Batch [100][550]\t Training Loss 82.7178\t Accuracy 0.9090\n",
            "Epoch [6][20]\t Batch [150][550]\t Training Loss 87.1921\t Accuracy 0.9050\n",
            "Epoch [6][20]\t Batch [200][550]\t Training Loss 86.8657\t Accuracy 0.9058\n",
            "Epoch [6][20]\t Batch [250][550]\t Training Loss 86.1235\t Accuracy 0.9062\n",
            "Epoch [6][20]\t Batch [300][550]\t Training Loss 85.9103\t Accuracy 0.9060\n",
            "Epoch [6][20]\t Batch [350][550]\t Training Loss 86.5484\t Accuracy 0.9048\n",
            "Epoch [6][20]\t Batch [400][550]\t Training Loss 86.0561\t Accuracy 0.9048\n",
            "Epoch [6][20]\t Batch [450][550]\t Training Loss 85.6641\t Accuracy 0.9050\n",
            "Epoch [6][20]\t Batch [500][550]\t Training Loss 86.4002\t Accuracy 0.9044\n",
            "\n",
            "Epoch [6]\t Average training loss 86.8809\t Average training accuracy 0.9043\n",
            "Epoch [6]\t Average validation loss 69.9100\t Average validation accuracy 0.9302\n",
            "\n",
            "Epoch [7][20]\t Batch [0][550]\t Training Loss 71.0000\t Accuracy 0.9500\n",
            "Epoch [7][20]\t Batch [50][550]\t Training Loss 73.4314\t Accuracy 0.9175\n",
            "Epoch [7][20]\t Batch [100][550]\t Training Loss 79.7921\t Accuracy 0.9125\n",
            "Epoch [7][20]\t Batch [150][550]\t Training Loss 84.3940\t Accuracy 0.9085\n",
            "Epoch [7][20]\t Batch [200][550]\t Training Loss 83.5796\t Accuracy 0.9095\n",
            "Epoch [7][20]\t Batch [250][550]\t Training Loss 82.7211\t Accuracy 0.9099\n",
            "Epoch [7][20]\t Batch [300][550]\t Training Loss 82.2940\t Accuracy 0.9097\n",
            "Epoch [7][20]\t Batch [350][550]\t Training Loss 82.8604\t Accuracy 0.9087\n",
            "Epoch [7][20]\t Batch [400][550]\t Training Loss 82.4688\t Accuracy 0.9085\n",
            "Epoch [7][20]\t Batch [450][550]\t Training Loss 82.0898\t Accuracy 0.9086\n",
            "Epoch [7][20]\t Batch [500][550]\t Training Loss 82.8473\t Accuracy 0.9080\n",
            "\n",
            "Epoch [7]\t Average training loss 83.1809\t Average training accuracy 0.9079\n",
            "Epoch [7]\t Average validation loss 67.9500\t Average validation accuracy 0.9318\n",
            "\n",
            "Epoch [8][20]\t Batch [0][550]\t Training Loss 71.5000\t Accuracy 0.9400\n",
            "Epoch [8][20]\t Batch [50][550]\t Training Loss 69.9706\t Accuracy 0.9206\n",
            "Epoch [8][20]\t Batch [100][550]\t Training Loss 76.5347\t Accuracy 0.9157\n",
            "Epoch [8][20]\t Batch [150][550]\t Training Loss 80.9735\t Accuracy 0.9115\n",
            "Epoch [8][20]\t Batch [200][550]\t Training Loss 80.5448\t Accuracy 0.9121\n",
            "Epoch [8][20]\t Batch [250][550]\t Training Loss 79.8785\t Accuracy 0.9126\n",
            "Epoch [8][20]\t Batch [300][550]\t Training Loss 79.4203\t Accuracy 0.9125\n",
            "Epoch [8][20]\t Batch [350][550]\t Training Loss 80.2806\t Accuracy 0.9113\n",
            "Epoch [8][20]\t Batch [400][550]\t Training Loss 80.0037\t Accuracy 0.9111\n",
            "Epoch [8][20]\t Batch [450][550]\t Training Loss 79.4878\t Accuracy 0.9112\n",
            "Epoch [8][20]\t Batch [500][550]\t Training Loss 80.2495\t Accuracy 0.9108\n",
            "\n",
            "Epoch [8]\t Average training loss 80.5636\t Average training accuracy 0.9106\n",
            "Epoch [8]\t Average validation loss 64.1900\t Average validation accuracy 0.9352\n",
            "\n",
            "Epoch [9][20]\t Batch [0][550]\t Training Loss 71.5000\t Accuracy 0.9400\n",
            "Epoch [9][20]\t Batch [50][550]\t Training Loss 69.2059\t Accuracy 0.9216\n",
            "Epoch [9][20]\t Batch [100][550]\t Training Loss 75.3762\t Accuracy 0.9170\n",
            "Epoch [9][20]\t Batch [150][550]\t Training Loss 79.9238\t Accuracy 0.9125\n",
            "Epoch [9][20]\t Batch [200][550]\t Training Loss 79.5547\t Accuracy 0.9134\n",
            "Epoch [9][20]\t Batch [250][550]\t Training Loss 79.0657\t Accuracy 0.9137\n",
            "Epoch [9][20]\t Batch [300][550]\t Training Loss 78.6312\t Accuracy 0.9138\n",
            "Epoch [9][20]\t Batch [350][550]\t Training Loss 79.5242\t Accuracy 0.9127\n",
            "Epoch [9][20]\t Batch [400][550]\t Training Loss 79.1334\t Accuracy 0.9127\n",
            "Epoch [9][20]\t Batch [450][550]\t Training Loss 78.5188\t Accuracy 0.9126\n",
            "Epoch [9][20]\t Batch [500][550]\t Training Loss 79.2096\t Accuracy 0.9122\n",
            "\n",
            "Epoch [9]\t Average training loss 79.5300\t Average training accuracy 0.9121\n",
            "Epoch [9]\t Average validation loss 62.8700\t Average validation accuracy 0.9366\n",
            "\n",
            "Epoch [10][20]\t Batch [0][550]\t Training Loss 88.0000\t Accuracy 0.9400\n",
            "Epoch [10][20]\t Batch [50][550]\t Training Loss 68.0490\t Accuracy 0.9224\n",
            "Epoch [10][20]\t Batch [100][550]\t Training Loss 74.3861\t Accuracy 0.9181\n",
            "Epoch [10][20]\t Batch [150][550]\t Training Loss 78.7285\t Accuracy 0.9142\n",
            "Epoch [10][20]\t Batch [200][550]\t Training Loss 77.5896\t Accuracy 0.9154\n",
            "Epoch [10][20]\t Batch [250][550]\t Training Loss 77.2371\t Accuracy 0.9156\n",
            "Epoch [10][20]\t Batch [300][550]\t Training Loss 77.0050\t Accuracy 0.9154\n",
            "Epoch [10][20]\t Batch [350][550]\t Training Loss 77.9131\t Accuracy 0.9145\n",
            "Epoch [10][20]\t Batch [400][550]\t Training Loss 77.5885\t Accuracy 0.9144\n",
            "Epoch [10][20]\t Batch [450][550]\t Training Loss 76.8647\t Accuracy 0.9144\n",
            "Epoch [10][20]\t Batch [500][550]\t Training Loss 77.7924\t Accuracy 0.9138\n",
            "\n",
            "Epoch [10]\t Average training loss 78.0927\t Average training accuracy 0.9137\n",
            "Epoch [10]\t Average validation loss 61.0200\t Average validation accuracy 0.9380\n",
            "\n",
            "Epoch [11][20]\t Batch [0][550]\t Training Loss 88.0000\t Accuracy 0.9400\n",
            "Epoch [11][20]\t Batch [50][550]\t Training Loss 67.9706\t Accuracy 0.9237\n",
            "Epoch [11][20]\t Batch [100][550]\t Training Loss 73.4307\t Accuracy 0.9198\n",
            "Epoch [11][20]\t Batch [150][550]\t Training Loss 77.4768\t Accuracy 0.9158\n",
            "Epoch [11][20]\t Batch [200][550]\t Training Loss 76.6493\t Accuracy 0.9170\n",
            "Epoch [11][20]\t Batch [250][550]\t Training Loss 76.2649\t Accuracy 0.9170\n",
            "Epoch [11][20]\t Batch [300][550]\t Training Loss 75.7774\t Accuracy 0.9169\n",
            "Epoch [11][20]\t Batch [350][550]\t Training Loss 76.6182\t Accuracy 0.9160\n",
            "Epoch [11][20]\t Batch [400][550]\t Training Loss 76.2481\t Accuracy 0.9158\n",
            "Epoch [11][20]\t Batch [450][550]\t Training Loss 75.5443\t Accuracy 0.9159\n",
            "Epoch [11][20]\t Batch [500][550]\t Training Loss 76.4232\t Accuracy 0.9153\n",
            "\n",
            "Epoch [11]\t Average training loss 76.7873\t Average training accuracy 0.9151\n",
            "Epoch [11]\t Average validation loss 60.9400\t Average validation accuracy 0.9382\n",
            "\n",
            "Epoch [12][20]\t Batch [0][550]\t Training Loss 88.0000\t Accuracy 0.9400\n",
            "Epoch [12][20]\t Batch [50][550]\t Training Loss 67.7059\t Accuracy 0.9245\n",
            "Epoch [12][20]\t Batch [100][550]\t Training Loss 73.1386\t Accuracy 0.9211\n",
            "Epoch [12][20]\t Batch [150][550]\t Training Loss 76.9503\t Accuracy 0.9170\n",
            "Epoch [12][20]\t Batch [200][550]\t Training Loss 76.0100\t Accuracy 0.9182\n",
            "Epoch [12][20]\t Batch [250][550]\t Training Loss 75.3167\t Accuracy 0.9186\n",
            "Epoch [12][20]\t Batch [300][550]\t Training Loss 74.7558\t Accuracy 0.9184\n",
            "Epoch [12][20]\t Batch [350][550]\t Training Loss 75.4117\t Accuracy 0.9175\n",
            "Epoch [12][20]\t Batch [400][550]\t Training Loss 75.1384\t Accuracy 0.9173\n",
            "Epoch [12][20]\t Batch [450][550]\t Training Loss 74.5455\t Accuracy 0.9173\n",
            "Epoch [12][20]\t Batch [500][550]\t Training Loss 75.4591\t Accuracy 0.9166\n",
            "\n",
            "Epoch [12]\t Average training loss 75.7336\t Average training accuracy 0.9165\n",
            "Epoch [12]\t Average validation loss 60.3100\t Average validation accuracy 0.9390\n",
            "\n",
            "Epoch [13][20]\t Batch [0][550]\t Training Loss 87.5000\t Accuracy 0.9500\n",
            "Epoch [13][20]\t Batch [50][550]\t Training Loss 66.6569\t Accuracy 0.9253\n",
            "Epoch [13][20]\t Batch [100][550]\t Training Loss 72.3762\t Accuracy 0.9220\n",
            "Epoch [13][20]\t Batch [150][550]\t Training Loss 76.0596\t Accuracy 0.9179\n",
            "Epoch [13][20]\t Batch [200][550]\t Training Loss 74.9478\t Accuracy 0.9194\n",
            "Epoch [13][20]\t Batch [250][550]\t Training Loss 74.2211\t Accuracy 0.9197\n",
            "Epoch [13][20]\t Batch [300][550]\t Training Loss 73.7674\t Accuracy 0.9196\n",
            "Epoch [13][20]\t Batch [350][550]\t Training Loss 74.3917\t Accuracy 0.9187\n",
            "Epoch [13][20]\t Batch [400][550]\t Training Loss 74.1746\t Accuracy 0.9184\n",
            "Epoch [13][20]\t Batch [450][550]\t Training Loss 73.6120\t Accuracy 0.9185\n",
            "Epoch [13][20]\t Batch [500][550]\t Training Loss 74.5509\t Accuracy 0.9177\n",
            "\n",
            "Epoch [13]\t Average training loss 74.8255\t Average training accuracy 0.9176\n",
            "Epoch [13]\t Average validation loss 59.5400\t Average validation accuracy 0.9398\n",
            "\n",
            "Epoch [14][20]\t Batch [0][550]\t Training Loss 87.5000\t Accuracy 0.9500\n",
            "Epoch [14][20]\t Batch [50][550]\t Training Loss 64.9706\t Accuracy 0.9269\n",
            "Epoch [14][20]\t Batch [100][550]\t Training Loss 71.2525\t Accuracy 0.9232\n",
            "Epoch [14][20]\t Batch [150][550]\t Training Loss 74.9073\t Accuracy 0.9188\n",
            "Epoch [14][20]\t Batch [200][550]\t Training Loss 74.2612\t Accuracy 0.9200\n",
            "Epoch [14][20]\t Batch [250][550]\t Training Loss 73.6036\t Accuracy 0.9204\n",
            "Epoch [14][20]\t Batch [300][550]\t Training Loss 73.1063\t Accuracy 0.9203\n",
            "Epoch [14][20]\t Batch [350][550]\t Training Loss 73.7479\t Accuracy 0.9193\n",
            "Epoch [14][20]\t Batch [400][550]\t Training Loss 73.5399\t Accuracy 0.9190\n",
            "Epoch [14][20]\t Batch [450][550]\t Training Loss 73.0200\t Accuracy 0.9191\n",
            "Epoch [14][20]\t Batch [500][550]\t Training Loss 74.0040\t Accuracy 0.9184\n",
            "\n",
            "Epoch [14]\t Average training loss 74.1664\t Average training accuracy 0.9183\n",
            "Epoch [14]\t Average validation loss 58.7300\t Average validation accuracy 0.9402\n",
            "\n",
            "Epoch [15][20]\t Batch [0][550]\t Training Loss 87.5000\t Accuracy 0.9500\n",
            "Epoch [15][20]\t Batch [50][550]\t Training Loss 65.2157\t Accuracy 0.9273\n",
            "Epoch [15][20]\t Batch [100][550]\t Training Loss 71.0198\t Accuracy 0.9240\n",
            "Epoch [15][20]\t Batch [150][550]\t Training Loss 74.4901\t Accuracy 0.9195\n",
            "Epoch [15][20]\t Batch [200][550]\t Training Loss 73.7537\t Accuracy 0.9207\n",
            "Epoch [15][20]\t Batch [250][550]\t Training Loss 73.2072\t Accuracy 0.9211\n",
            "Epoch [15][20]\t Batch [300][550]\t Training Loss 72.7110\t Accuracy 0.9210\n",
            "Epoch [15][20]\t Batch [350][550]\t Training Loss 73.3319\t Accuracy 0.9200\n",
            "Epoch [15][20]\t Batch [400][550]\t Training Loss 73.0075\t Accuracy 0.9198\n",
            "Epoch [15][20]\t Batch [450][550]\t Training Loss 72.5543\t Accuracy 0.9199\n",
            "Epoch [15][20]\t Batch [500][550]\t Training Loss 73.3862\t Accuracy 0.9191\n",
            "\n",
            "Epoch [15]\t Average training loss 73.5909\t Average training accuracy 0.9191\n",
            "Epoch [15]\t Average validation loss 58.2900\t Average validation accuracy 0.9406\n",
            "\n",
            "Epoch [16][20]\t Batch [0][550]\t Training Loss 87.5000\t Accuracy 0.9500\n",
            "Epoch [16][20]\t Batch [50][550]\t Training Loss 65.2941\t Accuracy 0.9275\n",
            "Epoch [16][20]\t Batch [100][550]\t Training Loss 70.1980\t Accuracy 0.9244\n",
            "Epoch [16][20]\t Batch [150][550]\t Training Loss 73.5728\t Accuracy 0.9199\n",
            "Epoch [16][20]\t Batch [200][550]\t Training Loss 73.0498\t Accuracy 0.9211\n",
            "Epoch [16][20]\t Batch [250][550]\t Training Loss 72.5120\t Accuracy 0.9216\n",
            "Epoch [16][20]\t Batch [300][550]\t Training Loss 72.0166\t Accuracy 0.9214\n",
            "Epoch [16][20]\t Batch [350][550]\t Training Loss 72.5655\t Accuracy 0.9206\n",
            "Epoch [16][20]\t Batch [400][550]\t Training Loss 72.1409\t Accuracy 0.9206\n",
            "Epoch [16][20]\t Batch [450][550]\t Training Loss 71.6951\t Accuracy 0.9206\n",
            "Epoch [16][20]\t Batch [500][550]\t Training Loss 72.5679\t Accuracy 0.9198\n",
            "\n",
            "Epoch [16]\t Average training loss 72.8000\t Average training accuracy 0.9197\n",
            "Epoch [16]\t Average validation loss 57.8000\t Average validation accuracy 0.9414\n",
            "\n",
            "Epoch [17][20]\t Batch [0][550]\t Training Loss 87.5000\t Accuracy 0.9500\n",
            "Epoch [17][20]\t Batch [50][550]\t Training Loss 65.0098\t Accuracy 0.9282\n",
            "Epoch [17][20]\t Batch [100][550]\t Training Loss 70.0941\t Accuracy 0.9248\n",
            "Epoch [17][20]\t Batch [150][550]\t Training Loss 73.1987\t Accuracy 0.9204\n",
            "Epoch [17][20]\t Batch [200][550]\t Training Loss 72.6045\t Accuracy 0.9216\n",
            "Epoch [17][20]\t Batch [250][550]\t Training Loss 72.2450\t Accuracy 0.9219\n",
            "Epoch [17][20]\t Batch [300][550]\t Training Loss 71.5199\t Accuracy 0.9220\n",
            "Epoch [17][20]\t Batch [350][550]\t Training Loss 71.9943\t Accuracy 0.9212\n",
            "Epoch [17][20]\t Batch [400][550]\t Training Loss 71.5187\t Accuracy 0.9213\n",
            "Epoch [17][20]\t Batch [450][550]\t Training Loss 70.9135\t Accuracy 0.9214\n",
            "Epoch [17][20]\t Batch [500][550]\t Training Loss 71.7914\t Accuracy 0.9207\n",
            "\n",
            "Epoch [17]\t Average training loss 72.1082\t Average training accuracy 0.9205\n",
            "Epoch [17]\t Average validation loss 57.2300\t Average validation accuracy 0.9420\n",
            "\n",
            "Epoch [18][20]\t Batch [0][550]\t Training Loss 87.5000\t Accuracy 0.9500\n",
            "Epoch [18][20]\t Batch [50][550]\t Training Loss 63.4902\t Accuracy 0.9288\n",
            "Epoch [18][20]\t Batch [100][550]\t Training Loss 68.7673\t Accuracy 0.9252\n",
            "Epoch [18][20]\t Batch [150][550]\t Training Loss 72.2616\t Accuracy 0.9210\n",
            "Epoch [18][20]\t Batch [200][550]\t Training Loss 71.7363\t Accuracy 0.9222\n",
            "Epoch [18][20]\t Batch [250][550]\t Training Loss 71.2829\t Accuracy 0.9226\n",
            "Epoch [18][20]\t Batch [300][550]\t Training Loss 70.6844\t Accuracy 0.9226\n",
            "Epoch [18][20]\t Batch [350][550]\t Training Loss 71.2222\t Accuracy 0.9218\n",
            "Epoch [18][20]\t Batch [400][550]\t Training Loss 70.7469\t Accuracy 0.9219\n",
            "Epoch [18][20]\t Batch [450][550]\t Training Loss 70.2483\t Accuracy 0.9220\n",
            "Epoch [18][20]\t Batch [500][550]\t Training Loss 71.2046\t Accuracy 0.9212\n",
            "\n",
            "Epoch [18]\t Average training loss 71.3855\t Average training accuracy 0.9212\n",
            "Epoch [18]\t Average validation loss 57.0700\t Average validation accuracy 0.9422\n",
            "\n",
            "Epoch [19][20]\t Batch [0][550]\t Training Loss 87.5000\t Accuracy 0.9500\n",
            "Epoch [19][20]\t Batch [50][550]\t Training Loss 63.1667\t Accuracy 0.9290\n",
            "Epoch [19][20]\t Batch [100][550]\t Training Loss 68.5941\t Accuracy 0.9257\n",
            "Epoch [19][20]\t Batch [150][550]\t Training Loss 71.8411\t Accuracy 0.9216\n",
            "Epoch [19][20]\t Batch [200][550]\t Training Loss 71.5224\t Accuracy 0.9226\n",
            "Epoch [19][20]\t Batch [250][550]\t Training Loss 71.0458\t Accuracy 0.9230\n",
            "Epoch [19][20]\t Batch [300][550]\t Training Loss 70.5698\t Accuracy 0.9229\n",
            "Epoch [19][20]\t Batch [350][550]\t Training Loss 71.0855\t Accuracy 0.9221\n",
            "Epoch [19][20]\t Batch [400][550]\t Training Loss 70.5436\t Accuracy 0.9222\n",
            "Epoch [19][20]\t Batch [450][550]\t Training Loss 70.1031\t Accuracy 0.9222\n",
            "Epoch [19][20]\t Batch [500][550]\t Training Loss 71.0359\t Accuracy 0.9215\n",
            "\n",
            "Epoch [19]\t Average training loss 71.1900\t Average training accuracy 0.9215\n",
            "Epoch [19]\t Average validation loss 56.7500\t Average validation accuracy 0.9426\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVoe__dErdkw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "bbc87547-5036-413f-9be8-8e2923c41f80"
      },
      "source": [
        "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "WARNING:tensorflow:From /content/drive/My Drive/homework2/solver.py:110: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "The test accuracy is 0.9267.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "685AQ7Mbrdky",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 MLP with Euclidean Loss and ReLU Activation Function\n",
        "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Euclidean loss function.\n",
        "\n",
        "### TODO\n",
        "Before executing the following code, you should complete **layers/relu_layer.py**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQCHQdplrdkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from layers import ReLULayer\n",
        "\n",
        "reluMLP = Network()\n",
        "# TODO build ReLUMLP with FCLayer and ReLULayer\n",
        "reluMLP.add(FCLayer(784, 128))\n",
        "reluMLP.add(ReLULayer())\n",
        "reluMLP.add(FCLayer(128, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "UIy_JCpardk3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a238935-e487-4fad-bd9b-a5bc6c1313a6"
      },
      "source": [
        "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0][20]\t Batch [0][550]\t Training Loss 855.5000\t Accuracy 0.0400\n",
            "Epoch [0][20]\t Batch [50][550]\t Training Loss 403.6176\t Accuracy 0.5557\n",
            "Epoch [0][20]\t Batch [100][550]\t Training Loss 305.9257\t Accuracy 0.6621\n",
            "Epoch [0][20]\t Batch [150][550]\t Training Loss 261.5132\t Accuracy 0.7093\n",
            "Epoch [0][20]\t Batch [200][550]\t Training Loss 232.3408\t Accuracy 0.7420\n",
            "Epoch [0][20]\t Batch [250][550]\t Training Loss 211.1952\t Accuracy 0.7661\n",
            "Epoch [0][20]\t Batch [300][550]\t Training Loss 194.5000\t Accuracy 0.7838\n",
            "Epoch [0][20]\t Batch [350][550]\t Training Loss 183.2422\t Accuracy 0.7962\n",
            "Epoch [0][20]\t Batch [400][550]\t Training Loss 172.6633\t Accuracy 0.8070\n",
            "Epoch [0][20]\t Batch [450][550]\t Training Loss 165.0044\t Accuracy 0.8156\n",
            "Epoch [0][20]\t Batch [500][550]\t Training Loss 158.4810\t Accuracy 0.8231\n",
            "\n",
            "Epoch [0]\t Average training loss 152.5000\t Average training accuracy 0.8298\n",
            "Epoch [0]\t Average validation loss 69.6300\t Average validation accuracy 0.9224\n",
            "\n",
            "Epoch [1][20]\t Batch [0][550]\t Training Loss 97.5000\t Accuracy 0.9500\n",
            "Epoch [1][20]\t Batch [50][550]\t Training Loss 84.3725\t Accuracy 0.9086\n",
            "Epoch [1][20]\t Batch [100][550]\t Training Loss 89.2129\t Accuracy 0.9086\n",
            "Epoch [1][20]\t Batch [150][550]\t Training Loss 89.6490\t Accuracy 0.9060\n",
            "Epoch [1][20]\t Batch [200][550]\t Training Loss 87.4552\t Accuracy 0.9079\n",
            "Epoch [1][20]\t Batch [250][550]\t Training Loss 84.9203\t Accuracy 0.9098\n",
            "Epoch [1][20]\t Batch [300][550]\t Training Loss 83.9153\t Accuracy 0.9105\n",
            "Epoch [1][20]\t Batch [350][550]\t Training Loss 83.2165\t Accuracy 0.9108\n",
            "Epoch [1][20]\t Batch [400][550]\t Training Loss 81.9589\t Accuracy 0.9116\n",
            "Epoch [1][20]\t Batch [450][550]\t Training Loss 81.1574\t Accuracy 0.9124\n",
            "Epoch [1][20]\t Batch [500][550]\t Training Loss 80.6038\t Accuracy 0.9126\n",
            "\n",
            "Epoch [1]\t Average training loss 80.0527\t Average training accuracy 0.9132\n",
            "Epoch [1]\t Average validation loss 51.3100\t Average validation accuracy 0.9436\n",
            "\n",
            "Epoch [2][20]\t Batch [0][550]\t Training Loss 98.0000\t Accuracy 0.9400\n",
            "Epoch [2][20]\t Batch [50][550]\t Training Loss 65.4412\t Accuracy 0.9276\n",
            "Epoch [2][20]\t Batch [100][550]\t Training Loss 70.5396\t Accuracy 0.9264\n",
            "Epoch [2][20]\t Batch [150][550]\t Training Loss 71.6921\t Accuracy 0.9243\n",
            "Epoch [2][20]\t Batch [200][550]\t Training Loss 70.3234\t Accuracy 0.9255\n",
            "Epoch [2][20]\t Batch [250][550]\t Training Loss 68.7112\t Accuracy 0.9266\n",
            "Epoch [2][20]\t Batch [300][550]\t Training Loss 68.3970\t Accuracy 0.9268\n",
            "Epoch [2][20]\t Batch [350][550]\t Training Loss 68.0199\t Accuracy 0.9267\n",
            "Epoch [2][20]\t Batch [400][550]\t Training Loss 67.1397\t Accuracy 0.9272\n",
            "Epoch [2][20]\t Batch [450][550]\t Training Loss 66.8160\t Accuracy 0.9274\n",
            "Epoch [2][20]\t Batch [500][550]\t Training Loss 67.3293\t Accuracy 0.9269\n",
            "\n",
            "Epoch [2]\t Average training loss 67.3345\t Average training accuracy 0.9270\n",
            "Epoch [2]\t Average validation loss 43.7600\t Average validation accuracy 0.9510\n",
            "\n",
            "Epoch [3][20]\t Batch [0][550]\t Training Loss 97.5000\t Accuracy 0.9500\n",
            "Epoch [3][20]\t Batch [50][550]\t Training Loss 56.9412\t Accuracy 0.9371\n",
            "Epoch [3][20]\t Batch [100][550]\t Training Loss 60.6485\t Accuracy 0.9352\n",
            "Epoch [3][20]\t Batch [150][550]\t Training Loss 62.7417\t Accuracy 0.9333\n",
            "Epoch [3][20]\t Batch [200][550]\t Training Loss 61.9851\t Accuracy 0.9346\n",
            "Epoch [3][20]\t Batch [250][550]\t Training Loss 60.1195\t Accuracy 0.9356\n",
            "Epoch [3][20]\t Batch [300][550]\t Training Loss 60.1163\t Accuracy 0.9354\n",
            "Epoch [3][20]\t Batch [350][550]\t Training Loss 59.9658\t Accuracy 0.9354\n",
            "Epoch [3][20]\t Batch [400][550]\t Training Loss 59.2756\t Accuracy 0.9357\n",
            "Epoch [3][20]\t Batch [450][550]\t Training Loss 58.7395\t Accuracy 0.9359\n",
            "Epoch [3][20]\t Batch [500][550]\t Training Loss 59.5349\t Accuracy 0.9354\n",
            "\n",
            "Epoch [3]\t Average training loss 59.5673\t Average training accuracy 0.9353\n",
            "Epoch [3]\t Average validation loss 41.3800\t Average validation accuracy 0.9546\n",
            "\n",
            "Epoch [4][20]\t Batch [0][550]\t Training Loss 97.5000\t Accuracy 0.9500\n",
            "Epoch [4][20]\t Batch [50][550]\t Training Loss 51.1765\t Accuracy 0.9437\n",
            "Epoch [4][20]\t Batch [100][550]\t Training Loss 55.4158\t Accuracy 0.9407\n",
            "Epoch [4][20]\t Batch [150][550]\t Training Loss 57.0563\t Accuracy 0.9397\n",
            "Epoch [4][20]\t Batch [200][550]\t Training Loss 56.0199\t Accuracy 0.9410\n",
            "Epoch [4][20]\t Batch [250][550]\t Training Loss 54.8745\t Accuracy 0.9416\n",
            "Epoch [4][20]\t Batch [300][550]\t Training Loss 55.3422\t Accuracy 0.9412\n",
            "Epoch [4][20]\t Batch [350][550]\t Training Loss 54.9430\t Accuracy 0.9412\n",
            "Epoch [4][20]\t Batch [400][550]\t Training Loss 54.4626\t Accuracy 0.9415\n",
            "Epoch [4][20]\t Batch [450][550]\t Training Loss 54.2705\t Accuracy 0.9417\n",
            "Epoch [4][20]\t Batch [500][550]\t Training Loss 54.9820\t Accuracy 0.9413\n",
            "\n",
            "Epoch [4]\t Average training loss 55.0655\t Average training accuracy 0.9411\n",
            "Epoch [4]\t Average validation loss 39.6400\t Average validation accuracy 0.9564\n",
            "\n",
            "Epoch [5][20]\t Batch [0][550]\t Training Loss 97.5000\t Accuracy 0.9500\n",
            "Epoch [5][20]\t Batch [50][550]\t Training Loss 43.4216\t Accuracy 0.9496\n",
            "Epoch [5][20]\t Batch [100][550]\t Training Loss 49.8020\t Accuracy 0.9461\n",
            "Epoch [5][20]\t Batch [150][550]\t Training Loss 52.2483\t Accuracy 0.9450\n",
            "Epoch [5][20]\t Batch [200][550]\t Training Loss 51.5945\t Accuracy 0.9459\n",
            "Epoch [5][20]\t Batch [250][550]\t Training Loss 50.7331\t Accuracy 0.9462\n",
            "Epoch [5][20]\t Batch [300][550]\t Training Loss 51.3156\t Accuracy 0.9458\n",
            "Epoch [5][20]\t Batch [350][550]\t Training Loss 50.6197\t Accuracy 0.9459\n",
            "Epoch [5][20]\t Batch [400][550]\t Training Loss 50.4850\t Accuracy 0.9461\n",
            "Epoch [5][20]\t Batch [450][550]\t Training Loss 50.2439\t Accuracy 0.9462\n",
            "Epoch [5][20]\t Batch [500][550]\t Training Loss 51.0060\t Accuracy 0.9458\n",
            "\n",
            "Epoch [5]\t Average training loss 51.2955\t Average training accuracy 0.9456\n",
            "Epoch [5]\t Average validation loss 38.9400\t Average validation accuracy 0.9586\n",
            "\n",
            "Epoch [6][20]\t Batch [0][550]\t Training Loss 95.5000\t Accuracy 0.9600\n",
            "Epoch [6][20]\t Batch [50][550]\t Training Loss 39.9510\t Accuracy 0.9547\n",
            "Epoch [6][20]\t Batch [100][550]\t Training Loss 45.6436\t Accuracy 0.9506\n",
            "Epoch [6][20]\t Batch [150][550]\t Training Loss 48.3344\t Accuracy 0.9491\n",
            "Epoch [6][20]\t Batch [200][550]\t Training Loss 47.4527\t Accuracy 0.9501\n",
            "Epoch [6][20]\t Batch [250][550]\t Training Loss 47.0279\t Accuracy 0.9501\n",
            "Epoch [6][20]\t Batch [300][550]\t Training Loss 47.6096\t Accuracy 0.9495\n",
            "Epoch [6][20]\t Batch [350][550]\t Training Loss 46.8134\t Accuracy 0.9497\n",
            "Epoch [6][20]\t Batch [400][550]\t Training Loss 46.9589\t Accuracy 0.9497\n",
            "Epoch [6][20]\t Batch [450][550]\t Training Loss 46.8160\t Accuracy 0.9497\n",
            "Epoch [6][20]\t Batch [500][550]\t Training Loss 47.5180\t Accuracy 0.9494\n",
            "\n",
            "Epoch [6]\t Average training loss 47.8164\t Average training accuracy 0.9493\n",
            "Epoch [6]\t Average validation loss 37.4300\t Average validation accuracy 0.9596\n",
            "\n",
            "Epoch [7][20]\t Batch [0][550]\t Training Loss 60.5000\t Accuracy 0.9700\n",
            "Epoch [7][20]\t Batch [50][550]\t Training Loss 37.3431\t Accuracy 0.9567\n",
            "Epoch [7][20]\t Batch [100][550]\t Training Loss 42.5495\t Accuracy 0.9528\n",
            "Epoch [7][20]\t Batch [150][550]\t Training Loss 44.8179\t Accuracy 0.9517\n",
            "Epoch [7][20]\t Batch [200][550]\t Training Loss 44.5896\t Accuracy 0.9525\n",
            "Epoch [7][20]\t Batch [250][550]\t Training Loss 44.0398\t Accuracy 0.9526\n",
            "Epoch [7][20]\t Batch [300][550]\t Training Loss 44.9169\t Accuracy 0.9518\n",
            "Epoch [7][20]\t Batch [350][550]\t Training Loss 44.1068\t Accuracy 0.9521\n",
            "Epoch [7][20]\t Batch [400][550]\t Training Loss 44.5985\t Accuracy 0.9519\n",
            "Epoch [7][20]\t Batch [450][550]\t Training Loss 44.5532\t Accuracy 0.9519\n",
            "Epoch [7][20]\t Batch [500][550]\t Training Loss 45.3862\t Accuracy 0.9516\n",
            "\n",
            "Epoch [7]\t Average training loss 45.6836\t Average training accuracy 0.9515\n",
            "Epoch [7]\t Average validation loss 36.3400\t Average validation accuracy 0.9608\n",
            "\n",
            "Epoch [8][20]\t Batch [0][550]\t Training Loss 60.5000\t Accuracy 0.9700\n",
            "Epoch [8][20]\t Batch [50][550]\t Training Loss 36.8137\t Accuracy 0.9592\n",
            "Epoch [8][20]\t Batch [100][550]\t Training Loss 42.0693\t Accuracy 0.9550\n",
            "Epoch [8][20]\t Batch [150][550]\t Training Loss 43.4272\t Accuracy 0.9544\n",
            "Epoch [8][20]\t Batch [200][550]\t Training Loss 43.1841\t Accuracy 0.9551\n",
            "Epoch [8][20]\t Batch [250][550]\t Training Loss 42.6594\t Accuracy 0.9548\n",
            "Epoch [8][20]\t Batch [300][550]\t Training Loss 43.6478\t Accuracy 0.9539\n",
            "Epoch [8][20]\t Batch [350][550]\t Training Loss 42.6681\t Accuracy 0.9543\n",
            "Epoch [8][20]\t Batch [400][550]\t Training Loss 42.9838\t Accuracy 0.9540\n",
            "Epoch [8][20]\t Batch [450][550]\t Training Loss 42.9690\t Accuracy 0.9540\n",
            "Epoch [8][20]\t Batch [500][550]\t Training Loss 43.9022\t Accuracy 0.9537\n",
            "\n",
            "Epoch [8]\t Average training loss 44.1527\t Average training accuracy 0.9535\n",
            "Epoch [8]\t Average validation loss 34.0900\t Average validation accuracy 0.9634\n",
            "\n",
            "Epoch [9][20]\t Batch [0][550]\t Training Loss 60.5000\t Accuracy 0.9700\n",
            "Epoch [9][20]\t Batch [50][550]\t Training Loss 36.4020\t Accuracy 0.9614\n",
            "Epoch [9][20]\t Batch [100][550]\t Training Loss 40.6980\t Accuracy 0.9574\n",
            "Epoch [9][20]\t Batch [150][550]\t Training Loss 41.7914\t Accuracy 0.9566\n",
            "Epoch [9][20]\t Batch [200][550]\t Training Loss 41.9677\t Accuracy 0.9568\n",
            "Epoch [9][20]\t Batch [250][550]\t Training Loss 41.2908\t Accuracy 0.9567\n",
            "Epoch [9][20]\t Batch [300][550]\t Training Loss 42.2342\t Accuracy 0.9557\n",
            "Epoch [9][20]\t Batch [350][550]\t Training Loss 41.3561\t Accuracy 0.9562\n",
            "Epoch [9][20]\t Batch [400][550]\t Training Loss 41.6459\t Accuracy 0.9558\n",
            "Epoch [9][20]\t Batch [450][550]\t Training Loss 41.6297\t Accuracy 0.9558\n",
            "Epoch [9][20]\t Batch [500][550]\t Training Loss 42.5339\t Accuracy 0.9554\n",
            "\n",
            "Epoch [9]\t Average training loss 42.7709\t Average training accuracy 0.9553\n",
            "Epoch [9]\t Average validation loss 33.4200\t Average validation accuracy 0.9644\n",
            "\n",
            "Epoch [10][20]\t Batch [0][550]\t Training Loss 60.5000\t Accuracy 0.9700\n",
            "Epoch [10][20]\t Batch [50][550]\t Training Loss 34.7843\t Accuracy 0.9627\n",
            "Epoch [10][20]\t Batch [100][550]\t Training Loss 39.3515\t Accuracy 0.9593\n",
            "Epoch [10][20]\t Batch [150][550]\t Training Loss 39.9172\t Accuracy 0.9588\n",
            "Epoch [10][20]\t Batch [200][550]\t Training Loss 40.3458\t Accuracy 0.9586\n",
            "Epoch [10][20]\t Batch [250][550]\t Training Loss 39.5120\t Accuracy 0.9587\n",
            "Epoch [10][20]\t Batch [300][550]\t Training Loss 40.7508\t Accuracy 0.9576\n",
            "Epoch [10][20]\t Batch [350][550]\t Training Loss 40.0043\t Accuracy 0.9580\n",
            "Epoch [10][20]\t Batch [400][550]\t Training Loss 40.2918\t Accuracy 0.9576\n",
            "Epoch [10][20]\t Batch [450][550]\t Training Loss 40.3315\t Accuracy 0.9575\n",
            "Epoch [10][20]\t Batch [500][550]\t Training Loss 41.2176\t Accuracy 0.9571\n",
            "\n",
            "Epoch [10]\t Average training loss 41.3436\t Average training accuracy 0.9571\n",
            "Epoch [10]\t Average validation loss 33.6700\t Average validation accuracy 0.9656\n",
            "\n",
            "Epoch [11][20]\t Batch [0][550]\t Training Loss 60.5000\t Accuracy 0.9700\n",
            "Epoch [11][20]\t Batch [50][550]\t Training Loss 33.8922\t Accuracy 0.9643\n",
            "Epoch [11][20]\t Batch [100][550]\t Training Loss 38.5644\t Accuracy 0.9606\n",
            "Epoch [11][20]\t Batch [150][550]\t Training Loss 38.7318\t Accuracy 0.9603\n",
            "Epoch [11][20]\t Batch [200][550]\t Training Loss 39.1343\t Accuracy 0.9600\n",
            "Epoch [11][20]\t Batch [250][550]\t Training Loss 38.2191\t Accuracy 0.9602\n",
            "Epoch [11][20]\t Batch [300][550]\t Training Loss 39.4668\t Accuracy 0.9590\n",
            "Epoch [11][20]\t Batch [350][550]\t Training Loss 38.5926\t Accuracy 0.9594\n",
            "Epoch [11][20]\t Batch [400][550]\t Training Loss 38.7382\t Accuracy 0.9591\n",
            "Epoch [11][20]\t Batch [450][550]\t Training Loss 38.8348\t Accuracy 0.9590\n",
            "Epoch [11][20]\t Batch [500][550]\t Training Loss 39.7475\t Accuracy 0.9586\n",
            "\n",
            "Epoch [11]\t Average training loss 39.8491\t Average training accuracy 0.9585\n",
            "Epoch [11]\t Average validation loss 32.7700\t Average validation accuracy 0.9664\n",
            "\n",
            "Epoch [12][20]\t Batch [0][550]\t Training Loss 60.5000\t Accuracy 0.9700\n",
            "Epoch [12][20]\t Batch [50][550]\t Training Loss 32.6471\t Accuracy 0.9659\n",
            "Epoch [12][20]\t Batch [100][550]\t Training Loss 36.6089\t Accuracy 0.9623\n",
            "Epoch [12][20]\t Batch [150][550]\t Training Loss 37.1060\t Accuracy 0.9617\n",
            "Epoch [12][20]\t Batch [200][550]\t Training Loss 37.5249\t Accuracy 0.9614\n",
            "Epoch [12][20]\t Batch [250][550]\t Training Loss 36.6315\t Accuracy 0.9616\n",
            "Epoch [12][20]\t Batch [300][550]\t Training Loss 37.8073\t Accuracy 0.9604\n",
            "Epoch [12][20]\t Batch [350][550]\t Training Loss 37.1809\t Accuracy 0.9608\n",
            "Epoch [12][20]\t Batch [400][550]\t Training Loss 37.2082\t Accuracy 0.9605\n",
            "Epoch [12][20]\t Batch [450][550]\t Training Loss 37.3625\t Accuracy 0.9603\n",
            "Epoch [12][20]\t Batch [500][550]\t Training Loss 38.3902\t Accuracy 0.9598\n",
            "\n",
            "Epoch [12]\t Average training loss 38.4200\t Average training accuracy 0.9598\n",
            "Epoch [12]\t Average validation loss 32.3500\t Average validation accuracy 0.9668\n",
            "\n",
            "Epoch [13][20]\t Batch [0][550]\t Training Loss 60.5000\t Accuracy 0.9700\n",
            "Epoch [13][20]\t Batch [50][550]\t Training Loss 32.3922\t Accuracy 0.9663\n",
            "Epoch [13][20]\t Batch [100][550]\t Training Loss 35.6782\t Accuracy 0.9630\n",
            "Epoch [13][20]\t Batch [150][550]\t Training Loss 36.0762\t Accuracy 0.9625\n",
            "Epoch [13][20]\t Batch [200][550]\t Training Loss 36.6095\t Accuracy 0.9622\n",
            "Epoch [13][20]\t Batch [250][550]\t Training Loss 35.7032\t Accuracy 0.9625\n",
            "Epoch [13][20]\t Batch [300][550]\t Training Loss 36.6811\t Accuracy 0.9615\n",
            "Epoch [13][20]\t Batch [350][550]\t Training Loss 36.2108\t Accuracy 0.9618\n",
            "Epoch [13][20]\t Batch [400][550]\t Training Loss 36.2232\t Accuracy 0.9614\n",
            "Epoch [13][20]\t Batch [450][550]\t Training Loss 36.4235\t Accuracy 0.9613\n",
            "Epoch [13][20]\t Batch [500][550]\t Training Loss 37.6188\t Accuracy 0.9606\n",
            "\n",
            "Epoch [13]\t Average training loss 37.5627\t Average training accuracy 0.9606\n",
            "Epoch [13]\t Average validation loss 31.0200\t Average validation accuracy 0.9680\n",
            "\n",
            "Epoch [14][20]\t Batch [0][550]\t Training Loss 60.5000\t Accuracy 0.9700\n",
            "Epoch [14][20]\t Batch [50][550]\t Training Loss 30.5588\t Accuracy 0.9669\n",
            "Epoch [14][20]\t Batch [100][550]\t Training Loss 33.8366\t Accuracy 0.9642\n",
            "Epoch [14][20]\t Batch [150][550]\t Training Loss 34.2815\t Accuracy 0.9637\n",
            "Epoch [14][20]\t Batch [200][550]\t Training Loss 35.0448\t Accuracy 0.9633\n",
            "Epoch [14][20]\t Batch [250][550]\t Training Loss 34.3865\t Accuracy 0.9636\n",
            "Epoch [14][20]\t Batch [300][550]\t Training Loss 35.2392\t Accuracy 0.9627\n",
            "Epoch [14][20]\t Batch [350][550]\t Training Loss 34.9217\t Accuracy 0.9629\n",
            "Epoch [14][20]\t Batch [400][550]\t Training Loss 34.9539\t Accuracy 0.9626\n",
            "Epoch [14][20]\t Batch [450][550]\t Training Loss 35.1563\t Accuracy 0.9624\n",
            "Epoch [14][20]\t Batch [500][550]\t Training Loss 36.3982\t Accuracy 0.9617\n",
            "\n",
            "Epoch [14]\t Average training loss 36.4073\t Average training accuracy 0.9617\n",
            "Epoch [14]\t Average validation loss 31.1500\t Average validation accuracy 0.9688\n",
            "\n",
            "Epoch [15][20]\t Batch [0][550]\t Training Loss 60.5000\t Accuracy 0.9700\n",
            "Epoch [15][20]\t Batch [50][550]\t Training Loss 29.9314\t Accuracy 0.9671\n",
            "Epoch [15][20]\t Batch [100][550]\t Training Loss 33.4554\t Accuracy 0.9645\n",
            "Epoch [15][20]\t Batch [150][550]\t Training Loss 33.7252\t Accuracy 0.9643\n",
            "Epoch [15][20]\t Batch [200][550]\t Training Loss 34.6990\t Accuracy 0.9639\n",
            "Epoch [15][20]\t Batch [250][550]\t Training Loss 34.2510\t Accuracy 0.9640\n",
            "Epoch [15][20]\t Batch [300][550]\t Training Loss 35.0581\t Accuracy 0.9632\n",
            "Epoch [15][20]\t Batch [350][550]\t Training Loss 34.7692\t Accuracy 0.9634\n",
            "Epoch [15][20]\t Batch [400][550]\t Training Loss 34.7643\t Accuracy 0.9631\n",
            "Epoch [15][20]\t Batch [450][550]\t Training Loss 34.9989\t Accuracy 0.9629\n",
            "Epoch [15][20]\t Batch [500][550]\t Training Loss 36.2036\t Accuracy 0.9622\n",
            "\n",
            "Epoch [15]\t Average training loss 36.1818\t Average training accuracy 0.9622\n",
            "Epoch [15]\t Average validation loss 30.4600\t Average validation accuracy 0.9696\n",
            "\n",
            "Epoch [16][20]\t Batch [0][550]\t Training Loss 60.5000\t Accuracy 0.9700\n",
            "Epoch [16][20]\t Batch [50][550]\t Training Loss 29.8235\t Accuracy 0.9680\n",
            "Epoch [16][20]\t Batch [100][550]\t Training Loss 32.3515\t Accuracy 0.9656\n",
            "Epoch [16][20]\t Batch [150][550]\t Training Loss 33.0728\t Accuracy 0.9648\n",
            "Epoch [16][20]\t Batch [200][550]\t Training Loss 33.7612\t Accuracy 0.9645\n",
            "Epoch [16][20]\t Batch [250][550]\t Training Loss 33.1554\t Accuracy 0.9647\n",
            "Epoch [16][20]\t Batch [300][550]\t Training Loss 34.1595\t Accuracy 0.9638\n",
            "Epoch [16][20]\t Batch [350][550]\t Training Loss 33.9786\t Accuracy 0.9640\n",
            "Epoch [16][20]\t Batch [400][550]\t Training Loss 33.9763\t Accuracy 0.9638\n",
            "Epoch [16][20]\t Batch [450][550]\t Training Loss 34.2428\t Accuracy 0.9636\n",
            "Epoch [16][20]\t Batch [500][550]\t Training Loss 35.4062\t Accuracy 0.9630\n",
            "\n",
            "Epoch [16]\t Average training loss 35.3609\t Average training accuracy 0.9630\n",
            "Epoch [16]\t Average validation loss 30.2700\t Average validation accuracy 0.9700\n",
            "\n",
            "Epoch [17][20]\t Batch [0][550]\t Training Loss 60.5000\t Accuracy 0.9700\n",
            "Epoch [17][20]\t Batch [50][550]\t Training Loss 29.7451\t Accuracy 0.9680\n",
            "Epoch [17][20]\t Batch [100][550]\t Training Loss 32.0644\t Accuracy 0.9659\n",
            "Epoch [17][20]\t Batch [150][550]\t Training Loss 32.6325\t Accuracy 0.9652\n",
            "Epoch [17][20]\t Batch [200][550]\t Training Loss 33.2662\t Accuracy 0.9649\n",
            "Epoch [17][20]\t Batch [250][550]\t Training Loss 32.2649\t Accuracy 0.9654\n",
            "Epoch [17][20]\t Batch [300][550]\t Training Loss 33.2392\t Accuracy 0.9644\n",
            "Epoch [17][20]\t Batch [350][550]\t Training Loss 33.1553\t Accuracy 0.9645\n",
            "Epoch [17][20]\t Batch [400][550]\t Training Loss 33.1983\t Accuracy 0.9644\n",
            "Epoch [17][20]\t Batch [450][550]\t Training Loss 33.4013\t Accuracy 0.9643\n",
            "Epoch [17][20]\t Batch [500][550]\t Training Loss 34.6078\t Accuracy 0.9635\n",
            "\n",
            "Epoch [17]\t Average training loss 34.5682\t Average training accuracy 0.9637\n",
            "Epoch [17]\t Average validation loss 28.9700\t Average validation accuracy 0.9708\n",
            "\n",
            "Epoch [18][20]\t Batch [0][550]\t Training Loss 60.5000\t Accuracy 0.9700\n",
            "Epoch [18][20]\t Batch [50][550]\t Training Loss 29.2843\t Accuracy 0.9688\n",
            "Epoch [18][20]\t Batch [100][550]\t Training Loss 31.0594\t Accuracy 0.9668\n",
            "Epoch [18][20]\t Batch [150][550]\t Training Loss 31.8344\t Accuracy 0.9662\n",
            "Epoch [18][20]\t Batch [200][550]\t Training Loss 32.4229\t Accuracy 0.9658\n",
            "Epoch [18][20]\t Batch [250][550]\t Training Loss 31.3884\t Accuracy 0.9663\n",
            "Epoch [18][20]\t Batch [300][550]\t Training Loss 32.4718\t Accuracy 0.9653\n",
            "Epoch [18][20]\t Batch [350][550]\t Training Loss 32.4359\t Accuracy 0.9653\n",
            "Epoch [18][20]\t Batch [400][550]\t Training Loss 32.6010\t Accuracy 0.9651\n",
            "Epoch [18][20]\t Batch [450][550]\t Training Loss 32.7328\t Accuracy 0.9650\n",
            "Epoch [18][20]\t Batch [500][550]\t Training Loss 33.8683\t Accuracy 0.9643\n",
            "\n",
            "Epoch [18]\t Average training loss 33.8655\t Average training accuracy 0.9644\n",
            "Epoch [18]\t Average validation loss 28.8000\t Average validation accuracy 0.9710\n",
            "\n",
            "Epoch [19][20]\t Batch [0][550]\t Training Loss 60.5000\t Accuracy 0.9700\n",
            "Epoch [19][20]\t Batch [50][550]\t Training Loss 28.8039\t Accuracy 0.9694\n",
            "Epoch [19][20]\t Batch [100][550]\t Training Loss 30.6386\t Accuracy 0.9672\n",
            "Epoch [19][20]\t Batch [150][550]\t Training Loss 31.2450\t Accuracy 0.9669\n",
            "Epoch [19][20]\t Batch [200][550]\t Training Loss 31.8507\t Accuracy 0.9665\n",
            "Epoch [19][20]\t Batch [250][550]\t Training Loss 30.7311\t Accuracy 0.9670\n",
            "Epoch [19][20]\t Batch [300][550]\t Training Loss 31.7226\t Accuracy 0.9661\n",
            "Epoch [19][20]\t Batch [350][550]\t Training Loss 31.6125\t Accuracy 0.9661\n",
            "Epoch [19][20]\t Batch [400][550]\t Training Loss 31.8641\t Accuracy 0.9659\n",
            "Epoch [19][20]\t Batch [450][550]\t Training Loss 31.8404\t Accuracy 0.9658\n",
            "Epoch [19][20]\t Batch [500][550]\t Training Loss 33.0569\t Accuracy 0.9651\n",
            "\n",
            "Epoch [19]\t Average training loss 33.0082\t Average training accuracy 0.9652\n",
            "Epoch [19]\t Average validation loss 28.7800\t Average validation accuracy 0.9710\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMxHzXiRrdk5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "53ed9b72-eeff-4ba7-8e53-ffd3eadc937d"
      },
      "source": [
        "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "The test accuracy is 0.9622.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYcSsS3Nrdk8",
        "colab_type": "text"
      },
      "source": [
        "## Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziEgPzPIrdk9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "e6c47764-d733-42b6-fa23-3cd7b3680d14"
      },
      "source": [
        "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
        "                   'relu': [relu_loss, relu_acc]})"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIu36/3eHTKeaBVi1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niRKHiIhEosQhIiKRKHGIiEgk/x9ScT7TbqvC/gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdwe3xsXrdlB",
        "colab_type": "text"
      },
      "source": [
        "## 2. MLP with Softmax Cross-Entropy Loss\n",
        "In part-2, you need to train a MLP with **Softmax Cross-Entropy Loss**.  \n",
        "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively again.\n",
        "### TODO\n",
        "Before executing the following code, you should complete **criterion/softmax_cross_entropy_loss.py**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7_R3hLerdlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from criterion import SoftmaxCrossEntropyLossLayer\n",
        "\n",
        "criterion = SoftmaxCrossEntropyLossLayer()\n",
        "\n",
        "sgd = SGD(learning_rate_SGD, weight_decay)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afskb4LKrdlF",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 MLP with Softmax Cross-Entropy Loss and Sigmoid Activation Function\n",
        "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Softmax cross-entropy loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCaglJUOrdlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sigmoidMLP = Network()\n",
        "# Build MLP with FCLayer and SigmoidLayer\n",
        "# 128 is the number of hidden units, you can change by your own\n",
        "sigmoidMLP.add(FCLayer(784, 128))\n",
        "sigmoidMLP.add(SigmoidLayer())\n",
        "sigmoidMLP.add(FCLayer(128, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeH2HNmZrdlI",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "b0cAFsYGrdlJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb1a4fed-3b38-4515-fb36-9b4bd5f9c268"
      },
      "source": [
        "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.7864\t Accuracy 0.1400\n",
            "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.6777\t Accuracy 0.1125\n",
            "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.6215\t Accuracy 0.1127\n",
            "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.5709\t Accuracy 0.1143\n",
            "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.5310\t Accuracy 0.1173\n",
            "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.4983\t Accuracy 0.1190\n",
            "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.4710\t Accuracy 0.1226\n",
            "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.4476\t Accuracy 0.1258\n",
            "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.4276\t Accuracy 0.1287\n",
            "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.4099\t Accuracy 0.1325\n",
            "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.3939\t Accuracy 0.1370\n",
            "\n",
            "Epoch [0]\t Average training loss 2.3794\t Average training accuracy 0.1424\n",
            "Epoch [0]\t Average validation loss 2.2209\t Average validation accuracy 0.2146\n",
            "\n",
            "Epoch [1][20]\t Batch [0][550]\t Training Loss 2.2088\t Accuracy 0.2300\n",
            "Epoch [1][20]\t Batch [50][550]\t Training Loss 2.2115\t Accuracy 0.2249\n",
            "Epoch [1][20]\t Batch [100][550]\t Training Loss 2.2102\t Accuracy 0.2251\n",
            "Epoch [1][20]\t Batch [150][550]\t Training Loss 2.2055\t Accuracy 0.2363\n",
            "Epoch [1][20]\t Batch [200][550]\t Training Loss 2.2006\t Accuracy 0.2417\n",
            "Epoch [1][20]\t Batch [250][550]\t Training Loss 2.1956\t Accuracy 0.2487\n",
            "Epoch [1][20]\t Batch [300][550]\t Training Loss 2.1899\t Accuracy 0.2573\n",
            "Epoch [1][20]\t Batch [350][550]\t Training Loss 2.1865\t Accuracy 0.2628\n",
            "Epoch [1][20]\t Batch [400][550]\t Training Loss 2.1823\t Accuracy 0.2706\n",
            "Epoch [1][20]\t Batch [450][550]\t Training Loss 2.1785\t Accuracy 0.2787\n",
            "Epoch [1][20]\t Batch [500][550]\t Training Loss 2.1743\t Accuracy 0.2874\n",
            "\n",
            "Epoch [1]\t Average training loss 2.1700\t Average training accuracy 0.2977\n",
            "Epoch [1]\t Average validation loss 2.1169\t Average validation accuracy 0.4234\n",
            "\n",
            "Epoch [2][20]\t Batch [0][550]\t Training Loss 2.0980\t Accuracy 0.4300\n",
            "Epoch [2][20]\t Batch [50][550]\t Training Loss 2.1154\t Accuracy 0.4225\n",
            "Epoch [2][20]\t Batch [100][550]\t Training Loss 2.1144\t Accuracy 0.4266\n",
            "Epoch [2][20]\t Batch [150][550]\t Training Loss 2.1126\t Accuracy 0.4324\n",
            "Epoch [2][20]\t Batch [200][550]\t Training Loss 2.1102\t Accuracy 0.4375\n",
            "Epoch [2][20]\t Batch [250][550]\t Training Loss 2.1070\t Accuracy 0.4413\n",
            "Epoch [2][20]\t Batch [300][550]\t Training Loss 2.1029\t Accuracy 0.4504\n",
            "Epoch [2][20]\t Batch [350][550]\t Training Loss 2.1016\t Accuracy 0.4520\n",
            "Epoch [2][20]\t Batch [400][550]\t Training Loss 2.0988\t Accuracy 0.4567\n",
            "Epoch [2][20]\t Batch [450][550]\t Training Loss 2.0965\t Accuracy 0.4601\n",
            "Epoch [2][20]\t Batch [500][550]\t Training Loss 2.0937\t Accuracy 0.4646\n",
            "\n",
            "Epoch [2]\t Average training loss 2.0905\t Average training accuracy 0.4701\n",
            "Epoch [2]\t Average validation loss 2.0471\t Average validation accuracy 0.5538\n",
            "\n",
            "Epoch [3][20]\t Batch [0][550]\t Training Loss 2.0283\t Accuracy 0.5900\n",
            "Epoch [3][20]\t Batch [50][550]\t Training Loss 2.0492\t Accuracy 0.5506\n",
            "Epoch [3][20]\t Batch [100][550]\t Training Loss 2.0485\t Accuracy 0.5479\n",
            "Epoch [3][20]\t Batch [150][550]\t Training Loss 2.0480\t Accuracy 0.5460\n",
            "Epoch [3][20]\t Batch [200][550]\t Training Loss 2.0468\t Accuracy 0.5478\n",
            "Epoch [3][20]\t Batch [250][550]\t Training Loss 2.0443\t Accuracy 0.5484\n",
            "Epoch [3][20]\t Batch [300][550]\t Training Loss 2.0410\t Accuracy 0.5531\n",
            "Epoch [3][20]\t Batch [350][550]\t Training Loss 2.0407\t Accuracy 0.5520\n",
            "Epoch [3][20]\t Batch [400][550]\t Training Loss 2.0386\t Accuracy 0.5540\n",
            "Epoch [3][20]\t Batch [450][550]\t Training Loss 2.0372\t Accuracy 0.5551\n",
            "Epoch [3][20]\t Batch [500][550]\t Training Loss 2.0351\t Accuracy 0.5575\n",
            "\n",
            "Epoch [3]\t Average training loss 2.0326\t Average training accuracy 0.5609\n",
            "Epoch [3]\t Average validation loss 1.9941\t Average validation accuracy 0.6264\n",
            "\n",
            "Epoch [4][20]\t Batch [0][550]\t Training Loss 1.9764\t Accuracy 0.6600\n",
            "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.9989\t Accuracy 0.6149\n",
            "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.9985\t Accuracy 0.6093\n",
            "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.9990\t Accuracy 0.6059\n",
            "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.9987\t Accuracy 0.6055\n",
            "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.9968\t Accuracy 0.6040\n",
            "Epoch [4][20]\t Batch [300][550]\t Training Loss 1.9940\t Accuracy 0.6067\n",
            "Epoch [4][20]\t Batch [350][550]\t Training Loss 1.9946\t Accuracy 0.6046\n",
            "Epoch [4][20]\t Batch [400][550]\t Training Loss 1.9929\t Accuracy 0.6059\n",
            "Epoch [4][20]\t Batch [450][550]\t Training Loss 1.9922\t Accuracy 0.6063\n",
            "Epoch [4][20]\t Batch [500][550]\t Training Loss 1.9906\t Accuracy 0.6085\n",
            "\n",
            "Epoch [4]\t Average training loss 1.9887\t Average training accuracy 0.6112\n",
            "Epoch [4]\t Average validation loss 1.9541\t Average validation accuracy 0.6762\n",
            "\n",
            "Epoch [5][20]\t Batch [0][550]\t Training Loss 1.9373\t Accuracy 0.6800\n",
            "Epoch [5][20]\t Batch [50][550]\t Training Loss 1.9608\t Accuracy 0.6525\n",
            "Epoch [5][20]\t Batch [100][550]\t Training Loss 1.9607\t Accuracy 0.6449\n",
            "Epoch [5][20]\t Batch [150][550]\t Training Loss 1.9619\t Accuracy 0.6405\n",
            "Epoch [5][20]\t Batch [200][550]\t Training Loss 1.9623\t Accuracy 0.6402\n",
            "Epoch [5][20]\t Batch [250][550]\t Training Loss 1.9608\t Accuracy 0.6384\n",
            "Epoch [5][20]\t Batch [300][550]\t Training Loss 1.9585\t Accuracy 0.6393\n",
            "Epoch [5][20]\t Batch [350][550]\t Training Loss 1.9596\t Accuracy 0.6379\n",
            "Epoch [5][20]\t Batch [400][550]\t Training Loss 1.9584\t Accuracy 0.6391\n",
            "Epoch [5][20]\t Batch [450][550]\t Training Loss 1.9582\t Accuracy 0.6391\n",
            "Epoch [5][20]\t Batch [500][550]\t Training Loss 1.9570\t Accuracy 0.6404\n",
            "\n",
            "Epoch [5]\t Average training loss 1.9555\t Average training accuracy 0.6429\n",
            "Epoch [5]\t Average validation loss 1.9239\t Average validation accuracy 0.7010\n",
            "\n",
            "Epoch [6][20]\t Batch [0][550]\t Training Loss 1.9080\t Accuracy 0.7200\n",
            "Epoch [6][20]\t Batch [50][550]\t Training Loss 1.9321\t Accuracy 0.6796\n",
            "Epoch [6][20]\t Batch [100][550]\t Training Loss 1.9323\t Accuracy 0.6718\n",
            "Epoch [6][20]\t Batch [150][550]\t Training Loss 1.9341\t Accuracy 0.6660\n",
            "Epoch [6][20]\t Batch [200][550]\t Training Loss 1.9350\t Accuracy 0.6648\n",
            "Epoch [6][20]\t Batch [250][550]\t Training Loss 1.9338\t Accuracy 0.6624\n",
            "Epoch [6][20]\t Batch [300][550]\t Training Loss 1.9318\t Accuracy 0.6619\n",
            "Epoch [6][20]\t Batch [350][550]\t Training Loss 1.9334\t Accuracy 0.6606\n",
            "Epoch [6][20]\t Batch [400][550]\t Training Loss 1.9325\t Accuracy 0.6613\n",
            "Epoch [6][20]\t Batch [450][550]\t Training Loss 1.9326\t Accuracy 0.6612\n",
            "Epoch [6][20]\t Batch [500][550]\t Training Loss 1.9319\t Accuracy 0.6618\n",
            "\n",
            "Epoch [6]\t Average training loss 1.9307\t Average training accuracy 0.6635\n",
            "Epoch [6]\t Average validation loss 1.9016\t Average validation accuracy 0.7198\n",
            "\n",
            "Epoch [7][20]\t Batch [0][550]\t Training Loss 1.8862\t Accuracy 0.7200\n",
            "Epoch [7][20]\t Batch [50][550]\t Training Loss 1.9109\t Accuracy 0.6929\n",
            "Epoch [7][20]\t Batch [100][550]\t Training Loss 1.9112\t Accuracy 0.6865\n",
            "Epoch [7][20]\t Batch [150][550]\t Training Loss 1.9134\t Accuracy 0.6799\n",
            "Epoch [7][20]\t Batch [200][550]\t Training Loss 1.9147\t Accuracy 0.6775\n",
            "Epoch [7][20]\t Batch [250][550]\t Training Loss 1.9137\t Accuracy 0.6760\n",
            "Epoch [7][20]\t Batch [300][550]\t Training Loss 1.9121\t Accuracy 0.6751\n",
            "Epoch [7][20]\t Batch [350][550]\t Training Loss 1.9140\t Accuracy 0.6737\n",
            "Epoch [7][20]\t Batch [400][550]\t Training Loss 1.9133\t Accuracy 0.6746\n",
            "Epoch [7][20]\t Batch [450][550]\t Training Loss 1.9138\t Accuracy 0.6742\n",
            "Epoch [7][20]\t Batch [500][550]\t Training Loss 1.9132\t Accuracy 0.6746\n",
            "\n",
            "Epoch [7]\t Average training loss 1.9123\t Average training accuracy 0.6760\n",
            "Epoch [7]\t Average validation loss 1.8853\t Average validation accuracy 0.7302\n",
            "\n",
            "Epoch [8][20]\t Batch [0][550]\t Training Loss 1.8705\t Accuracy 0.7300\n",
            "Epoch [8][20]\t Batch [50][550]\t Training Loss 1.8954\t Accuracy 0.7027\n",
            "Epoch [8][20]\t Batch [100][550]\t Training Loss 1.8958\t Accuracy 0.6972\n",
            "Epoch [8][20]\t Batch [150][550]\t Training Loss 1.8983\t Accuracy 0.6903\n",
            "Epoch [8][20]\t Batch [200][550]\t Training Loss 1.8999\t Accuracy 0.6878\n",
            "Epoch [8][20]\t Batch [250][550]\t Training Loss 1.8992\t Accuracy 0.6861\n",
            "Epoch [8][20]\t Batch [300][550]\t Training Loss 1.8977\t Accuracy 0.6847\n",
            "Epoch [8][20]\t Batch [350][550]\t Training Loss 1.8999\t Accuracy 0.6832\n",
            "Epoch [8][20]\t Batch [400][550]\t Training Loss 1.8994\t Accuracy 0.6838\n",
            "Epoch [8][20]\t Batch [450][550]\t Training Loss 1.9001\t Accuracy 0.6833\n",
            "Epoch [8][20]\t Batch [500][550]\t Training Loss 1.8998\t Accuracy 0.6836\n",
            "\n",
            "Epoch [8]\t Average training loss 1.8990\t Average training accuracy 0.6848\n",
            "Epoch [8]\t Average validation loss 1.8737\t Average validation accuracy 0.7358\n",
            "\n",
            "Epoch [9][20]\t Batch [0][550]\t Training Loss 1.8593\t Accuracy 0.7400\n",
            "Epoch [9][20]\t Batch [50][550]\t Training Loss 1.8844\t Accuracy 0.7075\n",
            "Epoch [9][20]\t Batch [100][550]\t Training Loss 1.8849\t Accuracy 0.7037\n",
            "Epoch [9][20]\t Batch [150][550]\t Training Loss 1.8876\t Accuracy 0.6962\n",
            "Epoch [9][20]\t Batch [200][550]\t Training Loss 1.8895\t Accuracy 0.6932\n",
            "Epoch [9][20]\t Batch [250][550]\t Training Loss 1.8889\t Accuracy 0.6913\n",
            "Epoch [9][20]\t Batch [300][550]\t Training Loss 1.8876\t Accuracy 0.6904\n",
            "Epoch [9][20]\t Batch [350][550]\t Training Loss 1.8900\t Accuracy 0.6888\n",
            "Epoch [9][20]\t Batch [400][550]\t Training Loss 1.8896\t Accuracy 0.6892\n",
            "Epoch [9][20]\t Batch [450][550]\t Training Loss 1.8905\t Accuracy 0.6890\n",
            "Epoch [9][20]\t Batch [500][550]\t Training Loss 1.8903\t Accuracy 0.6891\n",
            "\n",
            "Epoch [9]\t Average training loss 1.8898\t Average training accuracy 0.6903\n",
            "Epoch [9]\t Average validation loss 1.8659\t Average validation accuracy 0.7396\n",
            "\n",
            "Epoch [10][20]\t Batch [0][550]\t Training Loss 1.8519\t Accuracy 0.7400\n",
            "Epoch [10][20]\t Batch [50][550]\t Training Loss 1.8769\t Accuracy 0.7096\n",
            "Epoch [10][20]\t Batch [100][550]\t Training Loss 1.8775\t Accuracy 0.7066\n",
            "Epoch [10][20]\t Batch [150][550]\t Training Loss 1.8803\t Accuracy 0.6999\n",
            "Epoch [10][20]\t Batch [200][550]\t Training Loss 1.8824\t Accuracy 0.6971\n",
            "Epoch [10][20]\t Batch [250][550]\t Training Loss 1.8819\t Accuracy 0.6951\n",
            "Epoch [10][20]\t Batch [300][550]\t Training Loss 1.8808\t Accuracy 0.6940\n",
            "Epoch [10][20]\t Batch [350][550]\t Training Loss 1.8834\t Accuracy 0.6920\n",
            "Epoch [10][20]\t Batch [400][550]\t Training Loss 1.8831\t Accuracy 0.6927\n",
            "Epoch [10][20]\t Batch [450][550]\t Training Loss 1.8841\t Accuracy 0.6925\n",
            "Epoch [10][20]\t Batch [500][550]\t Training Loss 1.8841\t Accuracy 0.6924\n",
            "\n",
            "Epoch [10]\t Average training loss 1.8837\t Average training accuracy 0.6934\n",
            "Epoch [10]\t Average validation loss 1.8610\t Average validation accuracy 0.7426\n",
            "\n",
            "Epoch [11][20]\t Batch [0][550]\t Training Loss 1.8472\t Accuracy 0.7600\n",
            "Epoch [11][20]\t Batch [50][550]\t Training Loss 1.8722\t Accuracy 0.7100\n",
            "Epoch [11][20]\t Batch [100][550]\t Training Loss 1.8728\t Accuracy 0.7075\n",
            "Epoch [11][20]\t Batch [150][550]\t Training Loss 1.8758\t Accuracy 0.7015\n",
            "Epoch [11][20]\t Batch [200][550]\t Training Loss 1.8780\t Accuracy 0.6989\n",
            "Epoch [11][20]\t Batch [250][550]\t Training Loss 1.8777\t Accuracy 0.6973\n",
            "Epoch [11][20]\t Batch [300][550]\t Training Loss 1.8767\t Accuracy 0.6958\n",
            "Epoch [11][20]\t Batch [350][550]\t Training Loss 1.8793\t Accuracy 0.6940\n",
            "Epoch [11][20]\t Batch [400][550]\t Training Loss 1.8791\t Accuracy 0.6947\n",
            "Epoch [11][20]\t Batch [450][550]\t Training Loss 1.8802\t Accuracy 0.6945\n",
            "Epoch [11][20]\t Batch [500][550]\t Training Loss 1.8803\t Accuracy 0.6944\n",
            "\n",
            "Epoch [11]\t Average training loss 1.8800\t Average training accuracy 0.6953\n",
            "Epoch [11]\t Average validation loss 1.8584\t Average validation accuracy 0.7418\n",
            "\n",
            "Epoch [12][20]\t Batch [0][550]\t Training Loss 1.8449\t Accuracy 0.7700\n",
            "Epoch [12][20]\t Batch [50][550]\t Training Loss 1.8698\t Accuracy 0.7096\n",
            "Epoch [12][20]\t Batch [100][550]\t Training Loss 1.8704\t Accuracy 0.7076\n",
            "Epoch [12][20]\t Batch [150][550]\t Training Loss 1.8734\t Accuracy 0.7023\n",
            "Epoch [12][20]\t Batch [200][550]\t Training Loss 1.8758\t Accuracy 0.7000\n",
            "Epoch [12][20]\t Batch [250][550]\t Training Loss 1.8755\t Accuracy 0.6978\n",
            "Epoch [12][20]\t Batch [300][550]\t Training Loss 1.8746\t Accuracy 0.6965\n",
            "Epoch [12][20]\t Batch [350][550]\t Training Loss 1.8773\t Accuracy 0.6948\n",
            "Epoch [12][20]\t Batch [400][550]\t Training Loss 1.8772\t Accuracy 0.6955\n",
            "Epoch [12][20]\t Batch [450][550]\t Training Loss 1.8784\t Accuracy 0.6953\n",
            "Epoch [12][20]\t Batch [500][550]\t Training Loss 1.8785\t Accuracy 0.6953\n",
            "\n",
            "Epoch [12]\t Average training loss 1.8783\t Average training accuracy 0.6959\n",
            "Epoch [12]\t Average validation loss 1.8576\t Average validation accuracy 0.7424\n",
            "\n",
            "Epoch [13][20]\t Batch [0][550]\t Training Loss 1.8442\t Accuracy 0.7700\n",
            "Epoch [13][20]\t Batch [50][550]\t Training Loss 1.8690\t Accuracy 0.7108\n",
            "Epoch [13][20]\t Batch [100][550]\t Training Loss 1.8696\t Accuracy 0.7082\n",
            "Epoch [13][20]\t Batch [150][550]\t Training Loss 1.8727\t Accuracy 0.7023\n",
            "Epoch [13][20]\t Batch [200][550]\t Training Loss 1.8752\t Accuracy 0.6997\n",
            "Epoch [13][20]\t Batch [250][550]\t Training Loss 1.8749\t Accuracy 0.6976\n",
            "Epoch [13][20]\t Batch [300][550]\t Training Loss 1.8741\t Accuracy 0.6960\n",
            "Epoch [13][20]\t Batch [350][550]\t Training Loss 1.8769\t Accuracy 0.6944\n",
            "Epoch [13][20]\t Batch [400][550]\t Training Loss 1.8768\t Accuracy 0.6950\n",
            "Epoch [13][20]\t Batch [450][550]\t Training Loss 1.8781\t Accuracy 0.6949\n",
            "Epoch [13][20]\t Batch [500][550]\t Training Loss 1.8783\t Accuracy 0.6948\n",
            "\n",
            "Epoch [13]\t Average training loss 1.8781\t Average training accuracy 0.6954\n",
            "Epoch [13]\t Average validation loss 1.8583\t Average validation accuracy 0.7406\n",
            "\n",
            "Epoch [14][20]\t Batch [0][550]\t Training Loss 1.8450\t Accuracy 0.7700\n",
            "Epoch [14][20]\t Batch [50][550]\t Training Loss 1.8696\t Accuracy 0.7100\n",
            "Epoch [14][20]\t Batch [100][550]\t Training Loss 1.8702\t Accuracy 0.7070\n",
            "Epoch [14][20]\t Batch [150][550]\t Training Loss 1.8733\t Accuracy 0.7017\n",
            "Epoch [14][20]\t Batch [200][550]\t Training Loss 1.8758\t Accuracy 0.6988\n",
            "Epoch [14][20]\t Batch [250][550]\t Training Loss 1.8756\t Accuracy 0.6965\n",
            "Epoch [14][20]\t Batch [300][550]\t Training Loss 1.8749\t Accuracy 0.6951\n",
            "Epoch [14][20]\t Batch [350][550]\t Training Loss 1.8777\t Accuracy 0.6934\n",
            "Epoch [14][20]\t Batch [400][550]\t Training Loss 1.8777\t Accuracy 0.6938\n",
            "Epoch [14][20]\t Batch [450][550]\t Training Loss 1.8790\t Accuracy 0.6936\n",
            "Epoch [14][20]\t Batch [500][550]\t Training Loss 1.8792\t Accuracy 0.6936\n",
            "\n",
            "Epoch [14]\t Average training loss 1.8791\t Average training accuracy 0.6941\n",
            "Epoch [14]\t Average validation loss 1.8600\t Average validation accuracy 0.7376\n",
            "\n",
            "Epoch [15][20]\t Batch [0][550]\t Training Loss 1.8468\t Accuracy 0.7600\n",
            "Epoch [15][20]\t Batch [50][550]\t Training Loss 1.8712\t Accuracy 0.7090\n",
            "Epoch [15][20]\t Batch [100][550]\t Training Loss 1.8718\t Accuracy 0.7053\n",
            "Epoch [15][20]\t Batch [150][550]\t Training Loss 1.8749\t Accuracy 0.7003\n",
            "Epoch [15][20]\t Batch [200][550]\t Training Loss 1.8775\t Accuracy 0.6973\n",
            "Epoch [15][20]\t Batch [250][550]\t Training Loss 1.8773\t Accuracy 0.6953\n",
            "Epoch [15][20]\t Batch [300][550]\t Training Loss 1.8767\t Accuracy 0.6938\n",
            "Epoch [15][20]\t Batch [350][550]\t Training Loss 1.8795\t Accuracy 0.6922\n",
            "Epoch [15][20]\t Batch [400][550]\t Training Loss 1.8795\t Accuracy 0.6924\n",
            "Epoch [15][20]\t Batch [450][550]\t Training Loss 1.8808\t Accuracy 0.6922\n",
            "Epoch [15][20]\t Batch [500][550]\t Training Loss 1.8811\t Accuracy 0.6922\n",
            "\n",
            "Epoch [15]\t Average training loss 1.8811\t Average training accuracy 0.6925\n",
            "Epoch [15]\t Average validation loss 1.8626\t Average validation accuracy 0.7368\n",
            "\n",
            "Epoch [16][20]\t Batch [0][550]\t Training Loss 1.8494\t Accuracy 0.7600\n",
            "Epoch [16][20]\t Batch [50][550]\t Training Loss 1.8736\t Accuracy 0.7065\n",
            "Epoch [16][20]\t Batch [100][550]\t Training Loss 1.8742\t Accuracy 0.7031\n",
            "Epoch [16][20]\t Batch [150][550]\t Training Loss 1.8773\t Accuracy 0.6981\n",
            "Epoch [16][20]\t Batch [200][550]\t Training Loss 1.8799\t Accuracy 0.6952\n",
            "Epoch [16][20]\t Batch [250][550]\t Training Loss 1.8798\t Accuracy 0.6930\n",
            "Epoch [16][20]\t Batch [300][550]\t Training Loss 1.8792\t Accuracy 0.6914\n",
            "Epoch [16][20]\t Batch [350][550]\t Training Loss 1.8820\t Accuracy 0.6896\n",
            "Epoch [16][20]\t Batch [400][550]\t Training Loss 1.8820\t Accuracy 0.6899\n",
            "Epoch [16][20]\t Batch [450][550]\t Training Loss 1.8833\t Accuracy 0.6897\n",
            "Epoch [16][20]\t Batch [500][550]\t Training Loss 1.8837\t Accuracy 0.6898\n",
            "\n",
            "Epoch [16]\t Average training loss 1.8837\t Average training accuracy 0.6902\n",
            "Epoch [16]\t Average validation loss 1.8658\t Average validation accuracy 0.7338\n",
            "\n",
            "Epoch [17][20]\t Batch [0][550]\t Training Loss 1.8527\t Accuracy 0.7600\n",
            "Epoch [17][20]\t Batch [50][550]\t Training Loss 1.8766\t Accuracy 0.7024\n",
            "Epoch [17][20]\t Batch [100][550]\t Training Loss 1.8772\t Accuracy 0.6992\n",
            "Epoch [17][20]\t Batch [150][550]\t Training Loss 1.8803\t Accuracy 0.6948\n",
            "Epoch [17][20]\t Batch [200][550]\t Training Loss 1.8829\t Accuracy 0.6921\n",
            "Epoch [17][20]\t Batch [250][550]\t Training Loss 1.8828\t Accuracy 0.6901\n",
            "Epoch [17][20]\t Batch [300][550]\t Training Loss 1.8823\t Accuracy 0.6887\n",
            "Epoch [17][20]\t Batch [350][550]\t Training Loss 1.8850\t Accuracy 0.6870\n",
            "Epoch [17][20]\t Batch [400][550]\t Training Loss 1.8851\t Accuracy 0.6870\n",
            "Epoch [17][20]\t Batch [450][550]\t Training Loss 1.8864\t Accuracy 0.6868\n",
            "Epoch [17][20]\t Batch [500][550]\t Training Loss 1.8868\t Accuracy 0.6870\n",
            "\n",
            "Epoch [17]\t Average training loss 1.8868\t Average training accuracy 0.6877\n",
            "Epoch [17]\t Average validation loss 1.8695\t Average validation accuracy 0.7302\n",
            "\n",
            "Epoch [18][20]\t Batch [0][550]\t Training Loss 1.8564\t Accuracy 0.7500\n",
            "Epoch [18][20]\t Batch [50][550]\t Training Loss 1.8801\t Accuracy 0.6986\n",
            "Epoch [18][20]\t Batch [100][550]\t Training Loss 1.8806\t Accuracy 0.6962\n",
            "Epoch [18][20]\t Batch [150][550]\t Training Loss 1.8837\t Accuracy 0.6921\n",
            "Epoch [18][20]\t Batch [200][550]\t Training Loss 1.8864\t Accuracy 0.6891\n",
            "Epoch [18][20]\t Batch [250][550]\t Training Loss 1.8863\t Accuracy 0.6872\n",
            "Epoch [18][20]\t Batch [300][550]\t Training Loss 1.8857\t Accuracy 0.6859\n",
            "Epoch [18][20]\t Batch [350][550]\t Training Loss 1.8885\t Accuracy 0.6843\n",
            "Epoch [18][20]\t Batch [400][550]\t Training Loss 1.8886\t Accuracy 0.6843\n",
            "Epoch [18][20]\t Batch [450][550]\t Training Loss 1.8899\t Accuracy 0.6841\n",
            "Epoch [18][20]\t Batch [500][550]\t Training Loss 1.8903\t Accuracy 0.6842\n",
            "\n",
            "Epoch [18]\t Average training loss 1.8904\t Average training accuracy 0.6849\n",
            "Epoch [18]\t Average validation loss 1.8735\t Average validation accuracy 0.7268\n",
            "\n",
            "Epoch [19][20]\t Batch [0][550]\t Training Loss 1.8604\t Accuracy 0.7500\n",
            "Epoch [19][20]\t Batch [50][550]\t Training Loss 1.8839\t Accuracy 0.6951\n",
            "Epoch [19][20]\t Batch [100][550]\t Training Loss 1.8844\t Accuracy 0.6935\n",
            "Epoch [19][20]\t Batch [150][550]\t Training Loss 1.8875\t Accuracy 0.6887\n",
            "Epoch [19][20]\t Batch [200][550]\t Training Loss 1.8901\t Accuracy 0.6856\n",
            "Epoch [19][20]\t Batch [250][550]\t Training Loss 1.8900\t Accuracy 0.6838\n",
            "Epoch [19][20]\t Batch [300][550]\t Training Loss 1.8895\t Accuracy 0.6822\n",
            "Epoch [19][20]\t Batch [350][550]\t Training Loss 1.8923\t Accuracy 0.6807\n",
            "Epoch [19][20]\t Batch [400][550]\t Training Loss 1.8923\t Accuracy 0.6808\n",
            "Epoch [19][20]\t Batch [450][550]\t Training Loss 1.8937\t Accuracy 0.6807\n",
            "Epoch [19][20]\t Batch [500][550]\t Training Loss 1.8941\t Accuracy 0.6809\n",
            "\n",
            "Epoch [19]\t Average training loss 1.8942\t Average training accuracy 0.6816\n",
            "Epoch [19]\t Average validation loss 1.8778\t Average validation accuracy 0.7220\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKv2QF37rdlL",
        "colab_type": "text"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQEatGV-rdlM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "30fbb4cc-fe29-47cb-f7b6-7d6f21646f8a"
      },
      "source": [
        "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "The test accuracy is 0.7009.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPHLBL_KrdlS",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 MLP with Softmax Cross-Entropy Loss and ReLU Activation Function\n",
        "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Softmax cross-entropy loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KsIeIezrdlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reluMLP = Network()\n",
        "# Build ReLUMLP with FCLayer and ReLULayer\n",
        "# 128 is the number of hidden units, you can change by your own\n",
        "reluMLP.add(FCLayer(784, 128))\n",
        "reluMLP.add(ReLULayer())\n",
        "reluMLP.add(FCLayer(128, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "njtbT6WsrdlV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "067c84a8-b0ee-4207-8a57-015715fd8d44"
      },
      "source": [
        "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.5536\t Accuracy 0.1200\n",
            "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.4553\t Accuracy 0.1241\n",
            "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.4091\t Accuracy 0.1304\n",
            "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.3659\t Accuracy 0.1448\n",
            "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.3231\t Accuracy 0.1631\n",
            "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.2818\t Accuracy 0.1841\n",
            "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.2426\t Accuracy 0.2071\n",
            "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.2092\t Accuracy 0.2252\n",
            "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.1769\t Accuracy 0.2435\n",
            "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.1474\t Accuracy 0.2605\n",
            "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.1182\t Accuracy 0.2776\n",
            "\n",
            "Epoch [0]\t Average training loss 2.0902\t Average training accuracy 0.2949\n",
            "Epoch [0]\t Average validation loss 1.7624\t Average validation accuracy 0.4928\n",
            "\n",
            "Epoch [1][20]\t Batch [0][550]\t Training Loss 1.7807\t Accuracy 0.3900\n",
            "Epoch [1][20]\t Batch [50][550]\t Training Loss 1.7698\t Accuracy 0.4827\n",
            "Epoch [1][20]\t Batch [100][550]\t Training Loss 1.7491\t Accuracy 0.4907\n",
            "Epoch [1][20]\t Batch [150][550]\t Training Loss 1.7330\t Accuracy 0.4989\n",
            "Epoch [1][20]\t Batch [200][550]\t Training Loss 1.7176\t Accuracy 0.5080\n",
            "Epoch [1][20]\t Batch [250][550]\t Training Loss 1.6973\t Accuracy 0.5199\n",
            "Epoch [1][20]\t Batch [300][550]\t Training Loss 1.6775\t Accuracy 0.5313\n",
            "Epoch [1][20]\t Batch [350][550]\t Training Loss 1.6647\t Accuracy 0.5388\n",
            "Epoch [1][20]\t Batch [400][550]\t Training Loss 1.6479\t Accuracy 0.5478\n",
            "Epoch [1][20]\t Batch [450][550]\t Training Loss 1.6338\t Accuracy 0.5559\n",
            "Epoch [1][20]\t Batch [500][550]\t Training Loss 1.6190\t Accuracy 0.5630\n",
            "\n",
            "Epoch [1]\t Average training loss 1.6037\t Average training accuracy 0.5716\n",
            "Epoch [1]\t Average validation loss 1.3973\t Average validation accuracy 0.6878\n",
            "\n",
            "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.4071\t Accuracy 0.7000\n",
            "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.4225\t Accuracy 0.6600\n",
            "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.4120\t Accuracy 0.6651\n",
            "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.4052\t Accuracy 0.6658\n",
            "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.3994\t Accuracy 0.6680\n",
            "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.3868\t Accuracy 0.6746\n",
            "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.3748\t Accuracy 0.6792\n",
            "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.3703\t Accuracy 0.6807\n",
            "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.3599\t Accuracy 0.6853\n",
            "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.3521\t Accuracy 0.6885\n",
            "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.3435\t Accuracy 0.6904\n",
            "\n",
            "Epoch [2]\t Average training loss 1.3338\t Average training accuracy 0.6950\n",
            "Epoch [2]\t Average validation loss 1.1786\t Average validation accuracy 0.7738\n",
            "\n",
            "Epoch [3][20]\t Batch [0][550]\t Training Loss 1.1917\t Accuracy 0.7500\n",
            "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.2146\t Accuracy 0.7375\n",
            "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.2100\t Accuracy 0.7395\n",
            "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.2084\t Accuracy 0.7389\n",
            "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.2073\t Accuracy 0.7380\n",
            "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.1989\t Accuracy 0.7408\n",
            "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.1912\t Accuracy 0.7425\n",
            "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.1909\t Accuracy 0.7419\n",
            "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.1841\t Accuracy 0.7446\n",
            "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.1798\t Accuracy 0.7465\n",
            "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.1746\t Accuracy 0.7471\n",
            "\n",
            "Epoch [3]\t Average training loss 1.1680\t Average training accuracy 0.7500\n",
            "Epoch [3]\t Average validation loss 1.0407\t Average validation accuracy 0.8170\n",
            "\n",
            "Epoch [4][20]\t Batch [0][550]\t Training Loss 1.0613\t Accuracy 0.8000\n",
            "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.0832\t Accuracy 0.7749\n",
            "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.0820\t Accuracy 0.7762\n",
            "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.0834\t Accuracy 0.7732\n",
            "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.0848\t Accuracy 0.7727\n",
            "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.0787\t Accuracy 0.7744\n",
            "Epoch [4][20]\t Batch [300][550]\t Training Loss 1.0734\t Accuracy 0.7762\n",
            "Epoch [4][20]\t Batch [350][550]\t Training Loss 1.0755\t Accuracy 0.7756\n",
            "Epoch [4][20]\t Batch [400][550]\t Training Loss 1.0708\t Accuracy 0.7774\n",
            "Epoch [4][20]\t Batch [450][550]\t Training Loss 1.0684\t Accuracy 0.7786\n",
            "Epoch [4][20]\t Batch [500][550]\t Training Loss 1.0653\t Accuracy 0.7790\n",
            "\n",
            "Epoch [4]\t Average training loss 1.0605\t Average training accuracy 0.7813\n",
            "Epoch [4]\t Average validation loss 0.9497\t Average validation accuracy 0.8396\n",
            "\n",
            "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.9776\t Accuracy 0.8200\n",
            "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.9961\t Accuracy 0.7988\n",
            "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.9972\t Accuracy 0.8001\n",
            "Epoch [5][20]\t Batch [150][550]\t Training Loss 1.0004\t Accuracy 0.7966\n",
            "Epoch [5][20]\t Batch [200][550]\t Training Loss 1.0030\t Accuracy 0.7953\n",
            "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.9984\t Accuracy 0.7966\n",
            "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.9947\t Accuracy 0.7980\n",
            "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.9982\t Accuracy 0.7975\n",
            "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.9948\t Accuracy 0.7992\n",
            "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.9937\t Accuracy 0.7997\n",
            "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.9919\t Accuracy 0.7999\n",
            "\n",
            "Epoch [5]\t Average training loss 0.9882\t Average training accuracy 0.8017\n",
            "Epoch [5]\t Average validation loss 0.8878\t Average validation accuracy 0.8548\n",
            "\n",
            "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.9219\t Accuracy 0.8300\n",
            "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.9366\t Accuracy 0.8161\n",
            "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.9393\t Accuracy 0.8158\n",
            "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.9437\t Accuracy 0.8122\n",
            "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.9470\t Accuracy 0.8112\n",
            "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.9433\t Accuracy 0.8114\n",
            "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.9405\t Accuracy 0.8119\n",
            "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.9449\t Accuracy 0.8112\n",
            "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.9424\t Accuracy 0.8128\n",
            "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.9421\t Accuracy 0.8131\n",
            "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.9412\t Accuracy 0.8128\n",
            "\n",
            "Epoch [6]\t Average training loss 0.9383\t Average training accuracy 0.8142\n",
            "Epoch [6]\t Average validation loss 0.8450\t Average validation accuracy 0.8648\n",
            "\n",
            "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.8836\t Accuracy 0.8300\n",
            "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.8952\t Accuracy 0.8282\n",
            "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.8990\t Accuracy 0.8261\n",
            "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.9041\t Accuracy 0.8219\n",
            "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.9079\t Accuracy 0.8221\n",
            "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.9048\t Accuracy 0.8221\n",
            "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.9027\t Accuracy 0.8220\n",
            "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.9077\t Accuracy 0.8215\n",
            "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.9058\t Accuracy 0.8226\n",
            "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.9060\t Accuracy 0.8227\n",
            "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.9057\t Accuracy 0.8223\n",
            "\n",
            "Epoch [7]\t Average training loss 0.9033\t Average training accuracy 0.8235\n",
            "Epoch [7]\t Average validation loss 0.8151\t Average validation accuracy 0.8716\n",
            "\n",
            "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.8567\t Accuracy 0.8400\n",
            "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.8661\t Accuracy 0.8343\n",
            "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.8706\t Accuracy 0.8331\n",
            "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.8763\t Accuracy 0.8289\n",
            "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.8803\t Accuracy 0.8291\n",
            "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.8776\t Accuracy 0.8293\n",
            "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.8760\t Accuracy 0.8295\n",
            "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.8813\t Accuracy 0.8290\n",
            "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.8799\t Accuracy 0.8299\n",
            "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.8805\t Accuracy 0.8301\n",
            "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.8806\t Accuracy 0.8297\n",
            "\n",
            "Epoch [8]\t Average training loss 0.8786\t Average training accuracy 0.8307\n",
            "Epoch [8]\t Average validation loss 0.7941\t Average validation accuracy 0.8762\n",
            "\n",
            "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.8378\t Accuracy 0.8500\n",
            "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.8454\t Accuracy 0.8414\n",
            "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.8504\t Accuracy 0.8396\n",
            "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.8565\t Accuracy 0.8350\n",
            "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.8607\t Accuracy 0.8349\n",
            "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.8583\t Accuracy 0.8351\n",
            "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.8571\t Accuracy 0.8352\n",
            "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.8626\t Accuracy 0.8344\n",
            "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.8615\t Accuracy 0.8351\n",
            "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.8624\t Accuracy 0.8351\n",
            "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.8627\t Accuracy 0.8348\n",
            "\n",
            "Epoch [9]\t Average training loss 0.8611\t Average training accuracy 0.8357\n",
            "Epoch [9]\t Average validation loss 0.7793\t Average validation accuracy 0.8802\n",
            "\n",
            "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.8245\t Accuracy 0.8500\n",
            "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.8307\t Accuracy 0.8455\n",
            "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.8360\t Accuracy 0.8439\n",
            "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.8424\t Accuracy 0.8395\n",
            "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.8468\t Accuracy 0.8393\n",
            "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.8446\t Accuracy 0.8393\n",
            "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.8436\t Accuracy 0.8395\n",
            "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.8493\t Accuracy 0.8387\n",
            "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.8485\t Accuracy 0.8393\n",
            "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.8496\t Accuracy 0.8392\n",
            "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.8501\t Accuracy 0.8385\n",
            "\n",
            "Epoch [10]\t Average training loss 0.8487\t Average training accuracy 0.8393\n",
            "Epoch [10]\t Average validation loss 0.7690\t Average validation accuracy 0.8828\n",
            "\n",
            "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.8154\t Accuracy 0.8500\n",
            "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.8203\t Accuracy 0.8469\n",
            "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8259\t Accuracy 0.8461\n",
            "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8326\t Accuracy 0.8425\n",
            "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8370\t Accuracy 0.8422\n",
            "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8350\t Accuracy 0.8422\n",
            "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8342\t Accuracy 0.8425\n",
            "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8400\t Accuracy 0.8417\n",
            "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8394\t Accuracy 0.8423\n",
            "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8406\t Accuracy 0.8423\n",
            "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8413\t Accuracy 0.8417\n",
            "\n",
            "Epoch [11]\t Average training loss 0.8400\t Average training accuracy 0.8425\n",
            "Epoch [11]\t Average validation loss 0.7621\t Average validation accuracy 0.8854\n",
            "\n",
            "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.8091\t Accuracy 0.8600\n",
            "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.8132\t Accuracy 0.8512\n",
            "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8189\t Accuracy 0.8492\n",
            "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8258\t Accuracy 0.8454\n",
            "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8303\t Accuracy 0.8447\n",
            "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8284\t Accuracy 0.8446\n",
            "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8278\t Accuracy 0.8448\n",
            "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8337\t Accuracy 0.8442\n",
            "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8331\t Accuracy 0.8448\n",
            "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8345\t Accuracy 0.8447\n",
            "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8353\t Accuracy 0.8441\n",
            "\n",
            "Epoch [12]\t Average training loss 0.8342\t Average training accuracy 0.8449\n",
            "Epoch [12]\t Average validation loss 0.7576\t Average validation accuracy 0.8872\n",
            "\n",
            "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.8050\t Accuracy 0.8600\n",
            "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.8085\t Accuracy 0.8541\n",
            "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8142\t Accuracy 0.8520\n",
            "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8214\t Accuracy 0.8475\n",
            "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8259\t Accuracy 0.8470\n",
            "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8241\t Accuracy 0.8466\n",
            "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8236\t Accuracy 0.8469\n",
            "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8295\t Accuracy 0.8462\n",
            "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8291\t Accuracy 0.8468\n",
            "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8306\t Accuracy 0.8467\n",
            "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8314\t Accuracy 0.8460\n",
            "\n",
            "Epoch [13]\t Average training loss 0.8304\t Average training accuracy 0.8467\n",
            "Epoch [13]\t Average validation loss 0.7549\t Average validation accuracy 0.8888\n",
            "\n",
            "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.8024\t Accuracy 0.8600\n",
            "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.8055\t Accuracy 0.8575\n",
            "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.8113\t Accuracy 0.8549\n",
            "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8186\t Accuracy 0.8497\n",
            "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8231\t Accuracy 0.8488\n",
            "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8214\t Accuracy 0.8485\n",
            "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8210\t Accuracy 0.8489\n",
            "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8269\t Accuracy 0.8481\n",
            "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8266\t Accuracy 0.8485\n",
            "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8281\t Accuracy 0.8486\n",
            "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8290\t Accuracy 0.8479\n",
            "\n",
            "Epoch [14]\t Average training loss 0.8281\t Average training accuracy 0.8484\n",
            "Epoch [14]\t Average validation loss 0.7536\t Average validation accuracy 0.8898\n",
            "\n",
            "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.8009\t Accuracy 0.8600\n",
            "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.8039\t Accuracy 0.8590\n",
            "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.8097\t Accuracy 0.8558\n",
            "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8171\t Accuracy 0.8509\n",
            "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8216\t Accuracy 0.8499\n",
            "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8199\t Accuracy 0.8497\n",
            "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8196\t Accuracy 0.8501\n",
            "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8255\t Accuracy 0.8494\n",
            "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8252\t Accuracy 0.8498\n",
            "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8268\t Accuracy 0.8498\n",
            "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8278\t Accuracy 0.8493\n",
            "\n",
            "Epoch [15]\t Average training loss 0.8269\t Average training accuracy 0.8497\n",
            "Epoch [15]\t Average validation loss 0.7532\t Average validation accuracy 0.8890\n",
            "\n",
            "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.8003\t Accuracy 0.8600\n",
            "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.8032\t Accuracy 0.8600\n",
            "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.8090\t Accuracy 0.8567\n",
            "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8165\t Accuracy 0.8519\n",
            "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8210\t Accuracy 0.8506\n",
            "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8193\t Accuracy 0.8505\n",
            "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8190\t Accuracy 0.8511\n",
            "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8250\t Accuracy 0.8503\n",
            "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8248\t Accuracy 0.8507\n",
            "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8264\t Accuracy 0.8507\n",
            "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8274\t Accuracy 0.8503\n",
            "\n",
            "Epoch [16]\t Average training loss 0.8266\t Average training accuracy 0.8507\n",
            "Epoch [16]\t Average validation loss 0.7536\t Average validation accuracy 0.8900\n",
            "\n",
            "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.8003\t Accuracy 0.8600\n",
            "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.8032\t Accuracy 0.8612\n",
            "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.8090\t Accuracy 0.8578\n",
            "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8165\t Accuracy 0.8532\n",
            "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8211\t Accuracy 0.8521\n",
            "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8194\t Accuracy 0.8520\n",
            "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8192\t Accuracy 0.8526\n",
            "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8251\t Accuracy 0.8518\n",
            "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8249\t Accuracy 0.8521\n",
            "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8266\t Accuracy 0.8521\n",
            "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8276\t Accuracy 0.8517\n",
            "\n",
            "Epoch [17]\t Average training loss 0.8268\t Average training accuracy 0.8519\n",
            "Epoch [17]\t Average validation loss 0.7545\t Average validation accuracy 0.8910\n",
            "\n",
            "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.8007\t Accuracy 0.8600\n",
            "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.8037\t Accuracy 0.8618\n",
            "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.8095\t Accuracy 0.8582\n",
            "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8171\t Accuracy 0.8534\n",
            "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8216\t Accuracy 0.8525\n",
            "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8200\t Accuracy 0.8525\n",
            "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8197\t Accuracy 0.8530\n",
            "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8257\t Accuracy 0.8524\n",
            "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8256\t Accuracy 0.8528\n",
            "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8272\t Accuracy 0.8528\n",
            "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8282\t Accuracy 0.8523\n",
            "\n",
            "Epoch [18]\t Average training loss 0.8275\t Average training accuracy 0.8525\n",
            "Epoch [18]\t Average validation loss 0.7557\t Average validation accuracy 0.8912\n",
            "\n",
            "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.8014\t Accuracy 0.8600\n",
            "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.8046\t Accuracy 0.8633\n",
            "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.8104\t Accuracy 0.8592\n",
            "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8180\t Accuracy 0.8544\n",
            "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8225\t Accuracy 0.8537\n",
            "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8209\t Accuracy 0.8538\n",
            "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8207\t Accuracy 0.8542\n",
            "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8267\t Accuracy 0.8535\n",
            "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8265\t Accuracy 0.8538\n",
            "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8282\t Accuracy 0.8539\n",
            "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8292\t Accuracy 0.8533\n",
            "\n",
            "Epoch [19]\t Average training loss 0.8285\t Average training accuracy 0.8535\n",
            "Epoch [19]\t Average validation loss 0.7572\t Average validation accuracy 0.8914\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT3GXwPirdlY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "b0448a07-20d8-4012-c720-8caae01e838a"
      },
      "source": [
        "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "The test accuracy is 0.8655.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuCYigeRrdla",
        "colab_type": "text"
      },
      "source": [
        "## Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7ivp9yPrdlb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "7b33ab47-2a9d-466d-c56b-b147b9d4fb84"
      },
      "source": [
        "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
        "                   'relu': [relu_loss, relu_acc]})"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLiUIERGJSwlCRETiUoIQEZG4/h9yI6Xx/2JHYwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREQkJiUIERGJSQlCRERiUoIQEZGY/j+2uxc85I6X5AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVuwPzsordle",
        "colab_type": "text"
      },
      "source": [
        "### ~~You have finished homework2-mlp, congratulations!~~  \n",
        "\n",
        "**Next, according to the requirements 4) of report:**\n",
        "### **You need to construct a two-hidden-layer MLP, using any activation function and loss function.**\n",
        "\n",
        "**Note: Please insert some new cells blow (using '+' bottom in the toolbar) refer to above codes. Do not modify the former code directly.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBmnWu_03mHT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c8394a6c-4a1a-4cbd-c167-45e5e5312125"
      },
      "source": [
        "batch_size = 100\n",
        "max_epoch = 20\n",
        "init_std = 0.01\n",
        "learning_rate_SGD = 0.001\n",
        "weight_decay = 0.1\n",
        "disp_freq = 50\n",
        "\n",
        "myNetwork = Network()\n",
        "myNetwork.add(FCLayer(784, 256))\n",
        "myNetwork.add(ReLULayer())\n",
        "myNetwork.add(FCLayer(256, 128))\n",
        "myNetwork.add(ReLULayer())\n",
        "myNetwork.add(FCLayer(128, 10))\n",
        "\n",
        "criterion = SoftmaxCrossEntropyLossLayer()\n",
        "sgd = SGD(learning_rate_SGD, weight_decay)\n",
        "\n",
        "myNetwork, net_loss, net_acc = train(myNetwork, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
        "test(myNetwork, criterion, data_test, batch_size, disp_freq)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.6337\t Accuracy 0.1100\n",
            "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.5214\t Accuracy 0.1020\n",
            "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.4348\t Accuracy 0.1090\n",
            "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.3677\t Accuracy 0.1244\n",
            "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.3161\t Accuracy 0.1432\n",
            "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.2652\t Accuracy 0.1700\n",
            "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.2217\t Accuracy 0.1970\n",
            "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.1842\t Accuracy 0.2224\n",
            "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.1452\t Accuracy 0.2502\n",
            "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.1105\t Accuracy 0.2751\n",
            "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.0788\t Accuracy 0.2968\n",
            "\n",
            "Epoch [0]\t Average training loss 2.0465\t Average training accuracy 0.3202\n",
            "Epoch [0]\t Average validation loss 1.6545\t Average validation accuracy 0.5984\n",
            "\n",
            "Epoch [1][20]\t Batch [0][550]\t Training Loss 1.6014\t Accuracy 0.6200\n",
            "Epoch [1][20]\t Batch [50][550]\t Training Loss 1.6508\t Accuracy 0.5851\n",
            "Epoch [1][20]\t Batch [100][550]\t Training Loss 1.6322\t Accuracy 0.5930\n",
            "Epoch [1][20]\t Batch [150][550]\t Training Loss 1.6187\t Accuracy 0.5972\n",
            "Epoch [1][20]\t Batch [200][550]\t Training Loss 1.6060\t Accuracy 0.6036\n",
            "Epoch [1][20]\t Batch [250][550]\t Training Loss 1.5851\t Accuracy 0.6134\n",
            "Epoch [1][20]\t Batch [300][550]\t Training Loss 1.5672\t Accuracy 0.6207\n",
            "Epoch [1][20]\t Batch [350][550]\t Training Loss 1.5547\t Accuracy 0.6261\n",
            "Epoch [1][20]\t Batch [400][550]\t Training Loss 1.5357\t Accuracy 0.6332\n",
            "Epoch [1][20]\t Batch [450][550]\t Training Loss 1.5201\t Accuracy 0.6403\n",
            "Epoch [1][20]\t Batch [500][550]\t Training Loss 1.5048\t Accuracy 0.6452\n",
            "\n",
            "Epoch [1]\t Average training loss 1.4877\t Average training accuracy 0.6521\n",
            "Epoch [1]\t Average validation loss 1.2437\t Average validation accuracy 0.7618\n",
            "\n",
            "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.1972\t Accuracy 0.7800\n",
            "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.2635\t Accuracy 0.7412\n",
            "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.2557\t Accuracy 0.7396\n",
            "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.2532\t Accuracy 0.7344\n",
            "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.2491\t Accuracy 0.7340\n",
            "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.2364\t Accuracy 0.7371\n",
            "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.2266\t Accuracy 0.7388\n",
            "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.2228\t Accuracy 0.7397\n",
            "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.2116\t Accuracy 0.7425\n",
            "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.2035\t Accuracy 0.7457\n",
            "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.1949\t Accuracy 0.7475\n",
            "\n",
            "Epoch [2]\t Average training loss 1.1846\t Average training accuracy 0.7506\n",
            "Epoch [2]\t Average validation loss 1.0059\t Average validation accuracy 0.8220\n",
            "\n",
            "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.9809\t Accuracy 0.8100\n",
            "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.0411\t Accuracy 0.7947\n",
            "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.0396\t Accuracy 0.7916\n",
            "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.0427\t Accuracy 0.7858\n",
            "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.0427\t Accuracy 0.7858\n",
            "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.0346\t Accuracy 0.7880\n",
            "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.0293\t Accuracy 0.7891\n",
            "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.0302\t Accuracy 0.7886\n",
            "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.0237\t Accuracy 0.7906\n",
            "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.0199\t Accuracy 0.7918\n",
            "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.0154\t Accuracy 0.7931\n",
            "\n",
            "Epoch [3]\t Average training loss 1.0090\t Average training accuracy 0.7951\n",
            "Epoch [3]\t Average validation loss 0.8709\t Average validation accuracy 0.8526\n",
            "\n",
            "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.8634\t Accuracy 0.8200\n",
            "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.9153\t Accuracy 0.8239\n",
            "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.9173\t Accuracy 0.8187\n",
            "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.9234\t Accuracy 0.8142\n",
            "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.9253\t Accuracy 0.8139\n",
            "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.9198\t Accuracy 0.8147\n",
            "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.9171\t Accuracy 0.8154\n",
            "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.9205\t Accuracy 0.8143\n",
            "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.9166\t Accuracy 0.8156\n",
            "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.9152\t Accuracy 0.8163\n",
            "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.9130\t Accuracy 0.8169\n",
            "\n",
            "Epoch [4]\t Average training loss 0.9089\t Average training accuracy 0.8182\n",
            "Epoch [4]\t Average validation loss 0.7946\t Average validation accuracy 0.8686\n",
            "\n",
            "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.8008\t Accuracy 0.8500\n",
            "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.8437\t Accuracy 0.8384\n",
            "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.8475\t Accuracy 0.8335\n",
            "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.8550\t Accuracy 0.8289\n",
            "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.8578\t Accuracy 0.8293\n",
            "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.8538\t Accuracy 0.8301\n",
            "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.8524\t Accuracy 0.8306\n",
            "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.8571\t Accuracy 0.8294\n",
            "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.8548\t Accuracy 0.8304\n",
            "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.8546\t Accuracy 0.8308\n",
            "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.8537\t Accuracy 0.8310\n",
            "\n",
            "Epoch [5]\t Average training loss 0.8509\t Average training accuracy 0.8317\n",
            "Epoch [5]\t Average validation loss 0.7506\t Average validation accuracy 0.8798\n",
            "\n",
            "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.7655\t Accuracy 0.8800\n",
            "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.8019\t Accuracy 0.8482\n",
            "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.8066\t Accuracy 0.8422\n",
            "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.8149\t Accuracy 0.8383\n",
            "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.8181\t Accuracy 0.8390\n",
            "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.8150\t Accuracy 0.8400\n",
            "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.8144\t Accuracy 0.8405\n",
            "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.8198\t Accuracy 0.8395\n",
            "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.8184\t Accuracy 0.8402\n",
            "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.8189\t Accuracy 0.8403\n",
            "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.8188\t Accuracy 0.8403\n",
            "\n",
            "Epoch [6]\t Average training loss 0.8168\t Average training accuracy 0.8407\n",
            "Epoch [6]\t Average validation loss 0.7252\t Average validation accuracy 0.8848\n",
            "\n",
            "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.7460\t Accuracy 0.8800\n",
            "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.7773\t Accuracy 0.8539\n",
            "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.7827\t Accuracy 0.8478\n",
            "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.7913\t Accuracy 0.8441\n",
            "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.7949\t Accuracy 0.8447\n",
            "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.7923\t Accuracy 0.8454\n",
            "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.7922\t Accuracy 0.8459\n",
            "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.7979\t Accuracy 0.8452\n",
            "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.7971\t Accuracy 0.8456\n",
            "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.7981\t Accuracy 0.8460\n",
            "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.7984\t Accuracy 0.8459\n",
            "\n",
            "Epoch [7]\t Average training loss 0.7970\t Average training accuracy 0.8460\n",
            "Epoch [7]\t Average validation loss 0.7113\t Average validation accuracy 0.8888\n",
            "\n",
            "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.7369\t Accuracy 0.8800\n",
            "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.7635\t Accuracy 0.8588\n",
            "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.7693\t Accuracy 0.8536\n",
            "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.7781\t Accuracy 0.8493\n",
            "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.7818\t Accuracy 0.8504\n",
            "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.7796\t Accuracy 0.8508\n",
            "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.7798\t Accuracy 0.8511\n",
            "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.7857\t Accuracy 0.8501\n",
            "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.7852\t Accuracy 0.8504\n",
            "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.7865\t Accuracy 0.8506\n",
            "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.7871\t Accuracy 0.8503\n",
            "\n",
            "Epoch [8]\t Average training loss 0.7860\t Average training accuracy 0.8504\n",
            "Epoch [8]\t Average validation loss 0.7045\t Average validation accuracy 0.8916\n",
            "\n",
            "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.7334\t Accuracy 0.8700\n",
            "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.7563\t Accuracy 0.8618\n",
            "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.7624\t Accuracy 0.8563\n",
            "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.7712\t Accuracy 0.8541\n",
            "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.7750\t Accuracy 0.8548\n",
            "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.7731\t Accuracy 0.8549\n",
            "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.7734\t Accuracy 0.8551\n",
            "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.7795\t Accuracy 0.8540\n",
            "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.7793\t Accuracy 0.8544\n",
            "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.7807\t Accuracy 0.8543\n",
            "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.7815\t Accuracy 0.8541\n",
            "\n",
            "Epoch [9]\t Average training loss 0.7807\t Average training accuracy 0.8538\n",
            "Epoch [9]\t Average validation loss 0.7024\t Average validation accuracy 0.8942\n",
            "\n",
            "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.7330\t Accuracy 0.8700\n",
            "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.7536\t Accuracy 0.8643\n",
            "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.7598\t Accuracy 0.8596\n",
            "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.7686\t Accuracy 0.8568\n",
            "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.7725\t Accuracy 0.8574\n",
            "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.7708\t Accuracy 0.8576\n",
            "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.7713\t Accuracy 0.8576\n",
            "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.7775\t Accuracy 0.8566\n",
            "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.7774\t Accuracy 0.8568\n",
            "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.7789\t Accuracy 0.8567\n",
            "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.7799\t Accuracy 0.8564\n",
            "\n",
            "Epoch [10]\t Average training loss 0.7793\t Average training accuracy 0.8561\n",
            "Epoch [10]\t Average validation loss 0.7038\t Average validation accuracy 0.8934\n",
            "\n",
            "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.7359\t Accuracy 0.8800\n",
            "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.7545\t Accuracy 0.8653\n",
            "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.7606\t Accuracy 0.8614\n",
            "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.7693\t Accuracy 0.8582\n",
            "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.7734\t Accuracy 0.8588\n",
            "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.7718\t Accuracy 0.8592\n",
            "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.7723\t Accuracy 0.8593\n",
            "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.7786\t Accuracy 0.8585\n",
            "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.7786\t Accuracy 0.8587\n",
            "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.7803\t Accuracy 0.8585\n",
            "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.7814\t Accuracy 0.8580\n",
            "\n",
            "Epoch [11]\t Average training loss 0.7809\t Average training accuracy 0.8577\n",
            "Epoch [11]\t Average validation loss 0.7076\t Average validation accuracy 0.8932\n",
            "\n",
            "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.7414\t Accuracy 0.8800\n",
            "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.7577\t Accuracy 0.8673\n",
            "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.7638\t Accuracy 0.8628\n",
            "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.7725\t Accuracy 0.8595\n",
            "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.7766\t Accuracy 0.8599\n",
            "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.7751\t Accuracy 0.8605\n",
            "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.7757\t Accuracy 0.8606\n",
            "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.7820\t Accuracy 0.8598\n",
            "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.7821\t Accuracy 0.8601\n",
            "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.7839\t Accuracy 0.8598\n",
            "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.7850\t Accuracy 0.8593\n",
            "\n",
            "Epoch [12]\t Average training loss 0.7846\t Average training accuracy 0.8588\n",
            "Epoch [12]\t Average validation loss 0.7132\t Average validation accuracy 0.8938\n",
            "\n",
            "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.7486\t Accuracy 0.8800\n",
            "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.7626\t Accuracy 0.8661\n",
            "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.7687\t Accuracy 0.8623\n",
            "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.7773\t Accuracy 0.8597\n",
            "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.7815\t Accuracy 0.8603\n",
            "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.7800\t Accuracy 0.8610\n",
            "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.7806\t Accuracy 0.8612\n",
            "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.7869\t Accuracy 0.8604\n",
            "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.7871\t Accuracy 0.8605\n",
            "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.7889\t Accuracy 0.8602\n",
            "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.7901\t Accuracy 0.8597\n",
            "\n",
            "Epoch [13]\t Average training loss 0.7898\t Average training accuracy 0.8592\n",
            "Epoch [13]\t Average validation loss 0.7198\t Average validation accuracy 0.8932\n",
            "\n",
            "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.7568\t Accuracy 0.8700\n",
            "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.7686\t Accuracy 0.8661\n",
            "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.7746\t Accuracy 0.8628\n",
            "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.7831\t Accuracy 0.8603\n",
            "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.7873\t Accuracy 0.8606\n",
            "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.7858\t Accuracy 0.8612\n",
            "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.7865\t Accuracy 0.8612\n",
            "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.7928\t Accuracy 0.8605\n",
            "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.7930\t Accuracy 0.8610\n",
            "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.7948\t Accuracy 0.8606\n",
            "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.7960\t Accuracy 0.8602\n",
            "\n",
            "Epoch [14]\t Average training loss 0.7958\t Average training accuracy 0.8596\n",
            "Epoch [14]\t Average validation loss 0.7270\t Average validation accuracy 0.8938\n",
            "\n",
            "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.7652\t Accuracy 0.8500\n",
            "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.7752\t Accuracy 0.8665\n",
            "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.7811\t Accuracy 0.8628\n",
            "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.7895\t Accuracy 0.8601\n",
            "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.7937\t Accuracy 0.8605\n",
            "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.7922\t Accuracy 0.8612\n",
            "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.7929\t Accuracy 0.8612\n",
            "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.7992\t Accuracy 0.8605\n",
            "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.7994\t Accuracy 0.8610\n",
            "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8012\t Accuracy 0.8605\n",
            "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8024\t Accuracy 0.8602\n",
            "\n",
            "Epoch [15]\t Average training loss 0.8022\t Average training accuracy 0.8597\n",
            "Epoch [15]\t Average validation loss 0.7344\t Average validation accuracy 0.8936\n",
            "\n",
            "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.7734\t Accuracy 0.8600\n",
            "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.7820\t Accuracy 0.8682\n",
            "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.7879\t Accuracy 0.8644\n",
            "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.7962\t Accuracy 0.8611\n",
            "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8004\t Accuracy 0.8608\n",
            "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.7989\t Accuracy 0.8615\n",
            "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.7995\t Accuracy 0.8614\n",
            "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8059\t Accuracy 0.8606\n",
            "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8060\t Accuracy 0.8611\n",
            "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8078\t Accuracy 0.8607\n",
            "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8090\t Accuracy 0.8603\n",
            "\n",
            "Epoch [16]\t Average training loss 0.8089\t Average training accuracy 0.8597\n",
            "Epoch [16]\t Average validation loss 0.7419\t Average validation accuracy 0.8934\n",
            "\n",
            "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.7816\t Accuracy 0.8600\n",
            "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.7890\t Accuracy 0.8680\n",
            "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.7947\t Accuracy 0.8644\n",
            "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8029\t Accuracy 0.8610\n",
            "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8071\t Accuracy 0.8609\n",
            "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8056\t Accuracy 0.8615\n",
            "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8062\t Accuracy 0.8615\n",
            "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8125\t Accuracy 0.8607\n",
            "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8127\t Accuracy 0.8609\n",
            "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8144\t Accuracy 0.8605\n",
            "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8156\t Accuracy 0.8600\n",
            "\n",
            "Epoch [17]\t Average training loss 0.8155\t Average training accuracy 0.8595\n",
            "Epoch [17]\t Average validation loss 0.7492\t Average validation accuracy 0.8922\n",
            "\n",
            "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.7896\t Accuracy 0.8600\n",
            "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.7958\t Accuracy 0.8678\n",
            "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.8014\t Accuracy 0.8638\n",
            "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8095\t Accuracy 0.8602\n",
            "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8137\t Accuracy 0.8603\n",
            "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8121\t Accuracy 0.8608\n",
            "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8127\t Accuracy 0.8607\n",
            "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8189\t Accuracy 0.8601\n",
            "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8191\t Accuracy 0.8603\n",
            "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8209\t Accuracy 0.8599\n",
            "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8220\t Accuracy 0.8593\n",
            "\n",
            "Epoch [18]\t Average training loss 0.8219\t Average training accuracy 0.8588\n",
            "Epoch [18]\t Average validation loss 0.7563\t Average validation accuracy 0.8924\n",
            "\n",
            "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.7973\t Accuracy 0.8500\n",
            "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.8024\t Accuracy 0.8665\n",
            "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.8079\t Accuracy 0.8628\n",
            "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8159\t Accuracy 0.8592\n",
            "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8201\t Accuracy 0.8595\n",
            "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8185\t Accuracy 0.8600\n",
            "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8190\t Accuracy 0.8600\n",
            "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8252\t Accuracy 0.8594\n",
            "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8254\t Accuracy 0.8598\n",
            "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8271\t Accuracy 0.8593\n",
            "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8282\t Accuracy 0.8587\n",
            "\n",
            "Epoch [19]\t Average training loss 0.8281\t Average training accuracy 0.8583\n",
            "Epoch [19]\t Average validation loss 0.7632\t Average validation accuracy 0.8920\n",
            "\n",
            "Testing...\n",
            "The test accuracy is 0.8697.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZaaOt7c7eBr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "5dc9d81b-2c24-4a9b-f600-6ccac5a2fd43"
      },
      "source": [
        "plot_loss_and_acc({'Sigmoid': [net_loss, net_acc]})"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}